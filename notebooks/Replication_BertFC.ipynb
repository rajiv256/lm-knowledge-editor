{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-cu110.m82",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m82"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Replication BertFC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBya9GIKTAPi",
        "outputId": "19c56a99-8d9b-48ce-a574-9672288d8dc0"
      },
      "source": [
        "###reproducing the author's model###\n",
        "# Building models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "!pip install transformers\n",
        "from transformers import BertModel, BertTokenizer \n",
        "import tqdm\n",
        "\n",
        "# Building datasets\n",
        "# import src.preprocess\n",
        "# import os\n",
        "# import configs\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "id": "fBya9GIKTAPi",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh9dbMxPTAPq",
        "outputId": "6eba3fa9-70b3-4384-a40c-e595b8645d69"
      },
      "source": [
        "class BertFC(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"Downloads a BERT base uncased model and adds a linear layer on top of it\"\"\"\n",
        "        super(BertFC, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.classifier = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, ids, token_ids, mask):\n",
        "        \"\"\"\n",
        "        inputs: ids, token_ids, and mask each of dim = [bsz x seqlen]\n",
        "        returns: probability of a positive label, dim = [bsz]\n",
        "        \"\"\"\n",
        "        sequence_output = self.bert(input_ids = ids, token_type_ids = token_ids, attention_mask = mask)[0]\n",
        "        pooled_output = self.bert(input_ids = ids, token_type_ids = token_ids, attention_mask = mask)[1]\n",
        "        # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
        "        # sequence_output = nn.ReLU()(sequence_output)\n",
        "        # sequence_output = torch.tanh(sequence_output)\n",
        "        # linear_output = self.classifier(sequence_output[:, 0, :])\n",
        "        output = self.classifier(pooled_output)\n",
        "        return output.squeeze(0)\n",
        "\n",
        "# Build the model\n",
        "bert_fc_model = BertFC()\n"
      ],
      "id": "sh9dbMxPTAPq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIhPgVnWTZyV"
      },
      "source": [
        "class hypernetwork(nn.Module):                                                                       \n",
        "    def __init__(self,                                                                               \n",
        "                 n,                                                                                  \n",
        "                 m,                                                                                  \n",
        "                 input_size=768,  # verified                          \n",
        "                 hidden_size=128,  # verified                         \n",
        "                 linear_out=1024,  # verified                         \n",
        "                 num_layers=1):  # verified                           \n",
        "        \"\"\"                                                                                          \n",
        "        Args:                                                                                        \n",
        "            n:                                                                                       \n",
        "            m:                                                                                       \n",
        "            input_size:                                                                              \n",
        "            hidden_size:                                                                             \n",
        "            linear_out:                                                                              \n",
        "            num_layers:                                                                              \n",
        "        \"\"\"                                                                                          \n",
        "        super(hypernetwork, self).__init__()                          \n",
        "                                                                                                     \n",
        "        self.bilstm = nn.LSTM(input_size=input_size,                  \n",
        "                              hidden_size=hidden_size,                \n",
        "                              num_layers=num_layers,                  \n",
        "                              bidirectional=True,\n",
        "                              batch_first = True)     \n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size           \n",
        "        self.linear = nn.Linear(2*hidden_size, linear_out)            \n",
        "        self.alpha_linear = nn.Linear(linear_out, m)                 \n",
        "        self.beta_linear = nn.Linear(linear_out, m)                  \n",
        "        self.gamma_linear = nn.Linear(linear_out, n)                 \n",
        "        self.delta_linear = nn.Linear(linear_out, n)                 \n",
        "                                                                                                     \n",
        "        # TODO(rajiv): Maybe an intermediate layer?                   \n",
        "        self.eta_linear = nn.Linear(linear_out, 1)                   \n",
        "                                                                                                     \n",
        "    def forward(self, X, gradW):                                                                     \n",
        "        \"\"\"                                                                                          \n",
        "                                                                                                     \n",
        "        Args:                                                                                        \n",
        "            X: Vector(input [SEP] SUPPORTS [SEP] REFUTES)             \n",
        "            gradW: gradients of `finetuned_bert`.                     \n",
        "        Returns:                                                                                     \n",
        "                                                                                                     \n",
        "        \"\"\"                                                                                          \n",
        "        # TODO(rajiv): Make sure X is input + y +  a and not just input.                                                                                                     \n",
        "        # setting hidden states to allow lstm to run\n",
        "        hidden = torch.zeros((2*self.num_layers, 1, self.hidden_size))\n",
        "        cell   = torch.zeros((2*self.num_layers, 1, self.hidden_size))\n",
        "        # TODO: With L of 512, is the BiLSTM model going to forget parameters \n",
        "        # through time? Can we mask the bilstm input?\n",
        "        _, (hidden, _) = self.bilstm(X, (hidden, cell)) #hidden dim [2 x 1 x 128]\n",
        "        output = torch.tanh(self.linear(hidden.flatten())) #[1024]\n",
        "\n",
        "        alpha = self.alpha_linear(output) #[m]                          \n",
        "        beta = self.beta_linear(output)   #[m]                         \n",
        "        gamma = self.gamma_linear(output) #[n]                         \n",
        "        delta = self.delta_linear(output) #[n]                            \n",
        "        eta = self.eta_linear(output)     #[1]\n",
        "        print(\"gradW dim: \", gradW.size())\n",
        "        print(\"alpha dim: \", alpha.size())                               \n",
        "        print(\"beta dim: \", beta.size())\n",
        "        print(\"gamma dim: \", gamma.size())\n",
        "        print(\"delta dim: \", delta.size())\n",
        "        print(\"eta dim: \", eta.size())\n",
        "                                                                                                     \n",
        "        # TODO(rajiv): While computing *_hat, we are assuming that the first\n",
        "        # dimension would be the batch dimension. So we transpose the last two\n",
        "        # dimensions.                                                                                \n",
        "        alpha_hat = torch.outer(gamma, F.softmax(alpha))\n",
        "        beta_hat = torch.outer(delta, F.softmax(beta))\n",
        "        print(\"alpha_hat size: \", alpha_hat.size())\n",
        "        print(\"beta_hat size: \", beta_hat.size())\n",
        "                                                                                                     \n",
        "        delW = torch.sigmoid(eta) * ((alpha_hat * gradW) + beta_hat)\n",
        "        return delW\n",
        "                                                                        "
      ],
      "id": "vIhPgVnWTZyV",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glcNmfbETZp2"
      },
      "source": [
        "def get_attributes(module, names):\n",
        "  \"\"\"\"\n",
        "  inputs: Base Module and a list of module names\n",
        "  returns: the corresponding module\n",
        "  \"\"\"\n",
        "  if len(names) == 1:\n",
        "    return getattr(module, names[0])\n",
        "  else:\n",
        "    return get_attributes(getattr(module, names[0]), names[1:])\n",
        "\n",
        "class KnowledgeEditor(nn.Module):                                     \n",
        "  def __init__(self, BERT_model):                                   \n",
        "    \"\"\"                                                           \n",
        "    given a bert model, set up a hypernetwork                     \n",
        "    for every non-bias, embedding, or layer-norm                  \n",
        "    \"\"\"                                                           \n",
        "    super(KnowledgeEditor, self).__init__()                       \n",
        "    self.bert = BERT_model                                        \n",
        "    self.hyper_network_dict = nn.ModuleDict()                                  \n",
        "    for name, layer in BERT_model.named_parameters():            \n",
        "      if (\"LayerNorm\" not in name and \n",
        "          \"bias\" not in name and \n",
        "          \"embed\" not in name):\n",
        "      # Layers of size NxM \n",
        "        h = hypernetwork(layer.size()[0], layer.size()[1])\n",
        "        self.hyper_network_dict[str(name).replace(\".\", \"-\")] = h\n",
        "                                                                      \n",
        "  def forward(self, X_ids, X_type_ids, X_mask, A, \n",
        "              X_Y_A_ids, X_Y_A_type_ids, X_Y_A_mask):\n",
        "    \"\"\"takes tokenized input X, alternative answer A, and \n",
        "    hypernetwork specialized input X-<SEP>-Y-<SEP>-A                        \n",
        "    returns updated parameters in a dictionary of {Layer_name: delta_params}\n",
        "    \"\"\"                                 \n",
        "    # run the BERT model forward                                  \n",
        "    self.bert.zero_grad()\n",
        "    outputs = self.bert(X_ids, X_type_ids, X_mask)       \n",
        "    bert_loss = torch.nn.BCEWithLogitsLoss()\n",
        "    bert_losses = bert_loss(outputs, A)\n",
        "    bert_losses.backward() #get the gradients of BERT\n",
        "    #get pooled encoding from BERT\n",
        "    encoded_input = self.bert.bert(X_Y_A_ids, X_Y_A_type_ids, X_Y_A_mask)[0] \n",
        "\n",
        "    # Run the hypernetwork\n",
        "    h_outputs = {}\n",
        "    for layer, network in self.hyper_network_dict.items():\n",
        "      #get the grad for the layer in question\n",
        "      gradW = get_attributes(self.bert, layer.split(\"-\") + ['grad'])\n",
        "      #puts the new parameters in a dictionary of layer names \n",
        "      h_outputs[layer] = network(encoded_input, gradW)\n",
        "\n",
        "    return h_outputs\n",
        "\n",
        "ke = KnowledgeEditor(bert_fc_model)"
      ],
      "id": "glcNmfbETZp2",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wc7d78pTZgd"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "token = bert_tokenizer(\"this is a test\", max_length = 512, \n",
        "                       padding='max_length',\n",
        "                       truncation=True)\n",
        "x_id = torch.tensor(token.input_ids).unsqueeze(0)\n",
        "x_type_id = torch.tensor(token.token_type_ids).unsqueeze(0)\n",
        "x_mask = torch.tensor(token.attention_mask).unsqueeze(0)\n",
        "a = torch.FloatTensor([1])\n",
        "\n",
        "token = bert_tokenizer([\"this is a test [SEP] 1 [SEP] 0\"], max_length = 512, \n",
        "                       padding='max_length',\n",
        "                       truncation=True)\n",
        "\n",
        "xya_id = torch.tensor(token.input_ids)\n",
        "xya_type_id = torch.tensor(token.token_type_ids)\n",
        "xya_mask = torch.tensor(token.attention_mask)\n"
      ],
      "id": "3wc7d78pTZgd",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfMmepj_oU0G",
        "outputId": "9d99fac3-61ed-4d13-ca68-662b18ba8359"
      },
      "source": [
        "ke(x_id, x_type_id, x_mask, a, xya_id, xya_type_id, xya_mask)"
      ],
      "id": "qfMmepj_oU0G",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:69: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([3072, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([3072])\n",
            "delta dim:  torch.Size([3072])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([3072, 768])\n",
            "beta_hat size:  torch.Size([3072, 768])\n",
            "gradW dim:  torch.Size([768, 3072])\n",
            "alpha dim:  torch.Size([3072])\n",
            "beta dim:  torch.Size([3072])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 3072])\n",
            "beta_hat size:  torch.Size([768, 3072])\n",
            "gradW dim:  torch.Size([768, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([768])\n",
            "delta dim:  torch.Size([768])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([768, 768])\n",
            "beta_hat size:  torch.Size([768, 768])\n",
            "gradW dim:  torch.Size([1, 768])\n",
            "alpha dim:  torch.Size([768])\n",
            "beta dim:  torch.Size([768])\n",
            "gamma dim:  torch.Size([1])\n",
            "delta dim:  torch.Size([1])\n",
            "eta dim:  torch.Size([1])\n",
            "alpha_hat size:  torch.Size([1, 768])\n",
            "beta_hat size:  torch.Size([1, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert-encoder-layer-0-attention-output-dense-weight': tensor([[ 2.7598e-05,  2.8345e-05,  2.2954e-05,  ...,  2.5395e-05,\n",
              "           2.3192e-05,  2.4585e-05],\n",
              "         [ 1.0541e-04,  1.0833e-04,  8.7673e-05,  ...,  9.7038e-05,\n",
              "           8.8658e-05,  9.3853e-05],\n",
              "         [ 3.8623e-05,  3.9696e-05,  3.2131e-05,  ...,  3.5562e-05,\n",
              "           3.2487e-05,  3.4395e-05],\n",
              "         ...,\n",
              "         [-5.2005e-05, -5.3439e-05, -4.3249e-05,  ..., -4.7870e-05,\n",
              "          -4.3737e-05, -4.6297e-05],\n",
              "         [ 3.6744e-05,  3.7766e-05,  3.0567e-05,  ...,  3.3832e-05,\n",
              "           3.0908e-05,  3.2721e-05],\n",
              "         [-6.3287e-06, -6.5037e-06, -5.2650e-06,  ..., -5.8265e-06,\n",
              "          -5.3221e-06, -5.6371e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-0-attention-self-key-weight': tensor([[ 1.3809e-05,  1.1305e-05,  1.1670e-05,  ...,  9.1390e-06,\n",
              "           1.2878e-05,  1.2148e-05],\n",
              "         [ 5.2235e-06,  4.2732e-06,  4.4141e-06,  ...,  3.4518e-06,\n",
              "           4.8685e-06,  4.5886e-06],\n",
              "         [ 1.3627e-04,  1.1151e-04,  1.1518e-04,  ...,  9.0083e-05,\n",
              "           1.2705e-04,  1.1972e-04],\n",
              "         ...,\n",
              "         [-7.1423e-05, -5.8448e-05, -6.0374e-05,  ..., -4.7217e-05,\n",
              "          -6.6596e-05, -6.2746e-05],\n",
              "         [-1.1481e-04, -9.3954e-05, -9.7048e-05,  ..., -7.5899e-05,\n",
              "          -1.0704e-04, -1.0087e-04],\n",
              "         [ 9.5137e-05,  7.7856e-05,  8.0419e-05,  ...,  6.2897e-05,\n",
              "           8.8705e-05,  8.3579e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-0-attention-self-query-weight': tensor([[-1.6288e-05, -1.5897e-05, -1.4903e-05,  ..., -1.6057e-05,\n",
              "          -1.7550e-05, -1.4679e-05],\n",
              "         [-8.3144e-05, -8.1122e-05, -7.6055e-05,  ..., -8.1953e-05,\n",
              "          -8.9578e-05, -7.4916e-05],\n",
              "         [ 3.8580e-05,  3.7641e-05,  3.5288e-05,  ...,  3.8031e-05,\n",
              "           4.1568e-05,  3.4758e-05],\n",
              "         ...,\n",
              "         [ 3.7029e-05,  3.6133e-05,  3.3873e-05,  ...,  3.6505e-05,\n",
              "           3.9899e-05,  3.3364e-05],\n",
              "         [-5.9802e-05, -5.8347e-05, -5.4700e-05,  ..., -5.8951e-05,\n",
              "          -6.4434e-05, -5.3880e-05],\n",
              "         [ 4.2819e-05,  4.1779e-05,  3.9167e-05,  ...,  4.2210e-05,\n",
              "           4.6136e-05,  3.8580e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-0-attention-self-value-weight': tensor([[ 3.4129e-06,  2.9215e-06,  2.9286e-06,  ...,  3.1088e-06,\n",
              "           2.7001e-06,  3.0431e-06],\n",
              "         [ 1.9144e-05,  1.6438e-05,  1.6433e-05,  ...,  1.7498e-05,\n",
              "           1.5377e-05,  1.7145e-05],\n",
              "         [-1.2983e-04, -1.1148e-04, -1.1144e-04,  ..., -1.1867e-04,\n",
              "          -1.0428e-04, -1.1627e-04],\n",
              "         ...,\n",
              "         [ 1.3563e-05,  1.1646e-05,  1.1642e-05,  ...,  1.2398e-05,\n",
              "           1.0896e-05,  1.2147e-05],\n",
              "         [ 1.2388e-04,  1.0637e-04,  1.0633e-04,  ...,  1.1323e-04,\n",
              "           9.9503e-05,  1.1094e-04],\n",
              "         [ 1.3091e-04,  1.1240e-04,  1.1236e-04,  ...,  1.1967e-04,\n",
              "           1.0518e-04,  1.1725e-04]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-0-intermediate-dense-weight': tensor([[-2.0295e-05, -2.0723e-05, -1.8434e-05,  ..., -1.6298e-05,\n",
              "          -2.0301e-05, -2.4072e-05],\n",
              "         [-7.2321e-05, -7.3791e-05, -6.5653e-05,  ..., -5.8070e-05,\n",
              "          -7.2322e-05, -8.5700e-05],\n",
              "         [ 4.3797e-05,  4.4693e-05,  3.9761e-05,  ...,  3.5168e-05,\n",
              "           4.3799e-05,  5.1910e-05],\n",
              "         ...,\n",
              "         [-7.6681e-05, -7.8240e-05, -6.9612e-05,  ..., -6.1572e-05,\n",
              "          -7.6683e-05, -9.0868e-05],\n",
              "         [ 6.4761e-05,  6.6079e-05,  5.8791e-05,  ...,  5.2001e-05,\n",
              "           6.4764e-05,  7.6745e-05],\n",
              "         [ 2.5891e-06,  2.6293e-06,  2.3452e-06,  ...,  2.0744e-06,\n",
              "           2.5816e-06,  3.0484e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-0-output-dense-weight': tensor([[ 1.0156e-05,  9.3596e-06,  9.8039e-06,  ...,  9.9341e-06,\n",
              "           1.0311e-05,  1.1219e-05],\n",
              "         [-2.1781e-05, -2.0073e-05, -2.1026e-05,  ..., -2.1305e-05,\n",
              "          -2.2114e-05, -2.4062e-05],\n",
              "         [ 2.3924e-05,  2.2048e-05,  2.3094e-05,  ...,  2.3400e-05,\n",
              "           2.4290e-05,  2.6429e-05],\n",
              "         ...,\n",
              "         [-6.4015e-06, -5.8994e-06, -6.1799e-06,  ..., -6.2613e-06,\n",
              "          -6.4986e-06, -7.0719e-06],\n",
              "         [ 9.5244e-06,  8.7781e-06,  9.1945e-06,  ...,  9.3170e-06,\n",
              "           9.6708e-06,  1.0522e-05],\n",
              "         [-5.4228e-06, -4.9973e-06, -5.2346e-06,  ..., -5.3035e-06,\n",
              "          -5.5053e-06, -5.9905e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-1-attention-output-dense-weight': tensor([[-2.2770e-05, -2.3761e-05, -2.5379e-05,  ..., -2.5226e-05,\n",
              "          -2.5090e-05, -1.9505e-05],\n",
              "         [-6.6511e-05, -6.9400e-05, -7.4134e-05,  ..., -7.3702e-05,\n",
              "          -7.3300e-05, -5.6992e-05],\n",
              "         [ 7.6972e-05,  8.0315e-05,  8.5793e-05,  ...,  8.5296e-05,\n",
              "           8.4831e-05,  6.5955e-05],\n",
              "         ...,\n",
              "         [ 6.0051e-05,  6.2666e-05,  6.6937e-05,  ...,  6.6540e-05,\n",
              "           6.6175e-05,  5.1454e-05],\n",
              "         [ 7.8002e-05,  8.1387e-05,  8.6933e-05,  ...,  8.6418e-05,\n",
              "           8.5957e-05,  6.6813e-05],\n",
              "         [ 3.3496e-05,  3.4949e-05,  3.7331e-05,  ...,  3.7110e-05,\n",
              "           3.6912e-05,  2.8692e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-1-attention-self-key-weight': tensor([[ 7.1412e-05,  6.5045e-05,  7.2524e-05,  ...,  6.8994e-05,\n",
              "           7.1728e-05,  6.8078e-05],\n",
              "         [-6.8803e-05, -6.2657e-05, -6.9858e-05,  ..., -6.6473e-05,\n",
              "          -6.9087e-05, -6.5558e-05],\n",
              "         [ 2.6706e-05,  2.4326e-05,  2.7123e-05,  ...,  2.5803e-05,\n",
              "           2.6824e-05,  2.5461e-05],\n",
              "         ...,\n",
              "         [-3.4927e-06, -3.1810e-06, -3.5465e-06,  ..., -3.3751e-06,\n",
              "          -3.5077e-06, -3.3292e-06],\n",
              "         [-2.4219e-05, -2.2054e-05, -2.4590e-05,  ..., -2.3398e-05,\n",
              "          -2.4319e-05, -2.3079e-05],\n",
              "         [-3.3533e-05, -3.0540e-05, -3.4050e-05,  ..., -3.2397e-05,\n",
              "          -3.3676e-05, -3.1957e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-1-attention-self-query-weight': tensor([[ 1.7614e-05,  2.0211e-05,  2.2964e-05,  ...,  1.7656e-05,\n",
              "           1.6757e-05,  1.9868e-05],\n",
              "         [ 7.6391e-05,  8.7654e-05,  9.9594e-05,  ...,  7.6576e-05,\n",
              "           7.2677e-05,  8.6163e-05],\n",
              "         [-8.6524e-05, -9.9280e-05, -1.1280e-04,  ..., -8.6734e-05,\n",
              "          -8.2319e-05, -9.7591e-05],\n",
              "         ...,\n",
              "         [-2.7659e-05, -3.1749e-05, -3.6074e-05,  ..., -2.7729e-05,\n",
              "          -2.6319e-05, -3.1221e-05],\n",
              "         [-4.9063e-05, -5.6321e-05, -6.3994e-05,  ..., -4.9191e-05,\n",
              "          -4.6687e-05, -5.5386e-05],\n",
              "         [ 1.7686e-05,  2.0300e-05,  2.3065e-05,  ...,  1.7731e-05,\n",
              "           1.6830e-05,  1.9962e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-1-attention-self-value-weight': tensor([[-6.7529e-05, -5.9540e-05, -6.3172e-05,  ..., -5.5308e-05,\n",
              "          -5.8374e-05, -6.1021e-05],\n",
              "         [-7.4265e-05, -6.5490e-05, -6.9486e-05,  ..., -6.0827e-05,\n",
              "          -6.4210e-05, -6.7124e-05],\n",
              "         [ 3.4987e-05,  3.0852e-05,  3.2733e-05,  ...,  2.8655e-05,\n",
              "           3.0259e-05,  3.1621e-05],\n",
              "         ...,\n",
              "         [-2.1396e-05, -1.8854e-05, -2.0006e-05,  ..., -1.7517e-05,\n",
              "          -1.8487e-05, -1.9304e-05],\n",
              "         [ 1.2906e-05,  1.1386e-05,  1.2087e-05,  ...,  1.0570e-05,\n",
              "           1.1150e-05,  1.1682e-05],\n",
              "         [ 6.5157e-05,  5.7455e-05,  6.0969e-05,  ...,  5.3360e-05,\n",
              "           5.6315e-05,  5.8894e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-1-intermediate-dense-weight': tensor([[-3.7194e-05, -3.9278e-05, -3.7988e-05,  ..., -3.8683e-05,\n",
              "          -4.2119e-05, -3.7651e-05],\n",
              "         [ 5.1856e-06,  5.4765e-06,  5.2967e-06,  ...,  5.3933e-06,\n",
              "           5.8725e-06,  5.2504e-06],\n",
              "         [-1.2196e-05, -1.2884e-05, -1.2461e-05,  ..., -1.2689e-05,\n",
              "          -1.3816e-05, -1.2356e-05],\n",
              "         ...,\n",
              "         [-4.2661e-05, -4.5050e-05, -4.3570e-05,  ..., -4.4368e-05,\n",
              "          -4.8308e-05, -4.3182e-05],\n",
              "         [-1.1578e-04, -1.2226e-04, -1.1824e-04,  ..., -1.2041e-04,\n",
              "          -1.3110e-04, -1.1717e-04],\n",
              "         [-1.2721e-05, -1.3437e-05, -1.2994e-05,  ..., -1.3234e-05,\n",
              "          -1.4408e-05, -1.2885e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-1-output-dense-weight': tensor([[-1.2535e-05, -1.2335e-05, -1.0879e-05,  ..., -1.2293e-05,\n",
              "          -1.2390e-05, -1.2781e-05],\n",
              "         [-1.9059e-05, -1.8755e-05, -1.6541e-05,  ..., -1.8691e-05,\n",
              "          -1.8838e-05, -1.9434e-05],\n",
              "         [ 9.2310e-06,  9.0839e-06,  8.0116e-06,  ...,  9.0531e-06,\n",
              "           9.1243e-06,  9.4127e-06],\n",
              "         ...,\n",
              "         [ 1.3535e-05,  1.3320e-05,  1.1748e-05,  ...,  1.3275e-05,\n",
              "           1.3379e-05,  1.3802e-05],\n",
              "         [-2.4585e-05, -2.4194e-05, -2.1338e-05,  ..., -2.4111e-05,\n",
              "          -2.4301e-05, -2.5070e-05],\n",
              "         [-1.3447e-05, -1.3232e-05, -1.1670e-05,  ..., -1.3187e-05,\n",
              "          -1.3290e-05, -1.3711e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-10-attention-output-dense-weight': tensor([[ 2.2718e-05,  2.4639e-05,  2.6990e-05,  ...,  2.7511e-05,\n",
              "           2.6950e-05,  2.3929e-05],\n",
              "         [-4.2645e-05, -4.6255e-05, -5.0670e-05,  ..., -5.1652e-05,\n",
              "          -5.0593e-05, -4.4916e-05],\n",
              "         [-4.4379e-05, -4.8128e-05, -5.2716e-05,  ..., -5.3733e-05,\n",
              "          -5.2641e-05, -4.6745e-05],\n",
              "         ...,\n",
              "         [-3.4696e-06, -3.7629e-06, -4.1123e-06,  ..., -4.1821e-06,\n",
              "          -4.1035e-06, -3.6477e-06],\n",
              "         [ 1.2409e-05,  1.3431e-05,  1.4594e-05,  ...,  1.4815e-05,\n",
              "           1.4588e-05,  1.2983e-05],\n",
              "         [-6.1373e-05, -6.6565e-05, -7.2922e-05,  ..., -7.4332e-05,\n",
              "          -7.2811e-05, -6.4647e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-10-attention-self-key-weight': tensor([[ 1.9890e-06,  2.0409e-06,  1.8754e-06,  ...,  2.0721e-06,\n",
              "           1.9745e-06,  2.1921e-06],\n",
              "         [-3.7172e-05, -3.8199e-05, -3.4980e-05,  ..., -3.8744e-05,\n",
              "          -3.6899e-05, -4.0901e-05],\n",
              "         [-2.5362e-05, -2.6052e-05, -2.3867e-05,  ..., -2.6432e-05,\n",
              "          -2.5173e-05, -2.7903e-05],\n",
              "         ...,\n",
              "         [-1.4293e-05, -1.4695e-05, -1.3461e-05,  ..., -1.4897e-05,\n",
              "          -1.4194e-05, -1.5745e-05],\n",
              "         [ 9.3479e-05,  9.6037e-05,  8.7970e-05,  ...,  9.7426e-05,\n",
              "           9.2788e-05,  1.0286e-04],\n",
              "         [ 4.0586e-05,  4.1694e-05,  3.8197e-05,  ...,  4.2299e-05,\n",
              "           4.0286e-05,  4.4658e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-10-attention-self-query-weight': tensor([[ 8.7712e-05,  1.0007e-04,  9.6730e-05,  ...,  9.8355e-05,\n",
              "           8.4752e-05,  9.9698e-05],\n",
              "         [-2.2470e-05, -2.5650e-05, -2.4777e-05,  ..., -2.5199e-05,\n",
              "          -2.1706e-05, -2.5564e-05],\n",
              "         [-7.2074e-05, -8.2229e-05, -7.9480e-05,  ..., -8.0820e-05,\n",
              "          -6.9640e-05, -8.1922e-05],\n",
              "         ...,\n",
              "         [-3.1334e-05, -3.5754e-05, -3.4556e-05,  ..., -3.5136e-05,\n",
              "          -3.0279e-05, -3.5618e-05],\n",
              "         [-7.9729e-05, -9.0965e-05, -8.7921e-05,  ..., -8.9404e-05,\n",
              "          -7.7035e-05, -9.0629e-05],\n",
              "         [ 2.8689e-05,  3.2744e-05,  3.1638e-05,  ...,  3.2172e-05,\n",
              "           2.7720e-05,  3.2624e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-10-attention-self-value-weight': tensor([[-1.2526e-04, -1.4133e-04, -1.3549e-04,  ..., -1.3888e-04,\n",
              "          -1.4083e-04, -1.5370e-04],\n",
              "         [ 2.7321e-05,  3.0833e-05,  2.9546e-05,  ...,  3.0289e-05,\n",
              "           3.0722e-05,  3.3509e-05],\n",
              "         [ 4.9299e-06,  5.5638e-06,  5.3331e-06,  ...,  5.4661e-06,\n",
              "           5.5432e-06,  6.0500e-06],\n",
              "         ...,\n",
              "         [-4.0658e-05, -4.5873e-05, -4.3982e-05,  ..., -4.5080e-05,\n",
              "          -4.5710e-05, -4.9894e-05],\n",
              "         [ 1.1252e-04,  1.2696e-04,  1.2172e-04,  ...,  1.2476e-04,\n",
              "           1.2650e-04,  1.3808e-04],\n",
              "         [ 2.2776e-05,  2.5704e-05,  2.4627e-05,  ...,  2.5247e-05,\n",
              "           2.5612e-05,  2.7929e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-10-intermediate-dense-weight': tensor([[ 7.0154e-05,  7.1077e-05,  6.2802e-05,  ...,  5.8770e-05,\n",
              "           7.4200e-05,  7.3534e-05],\n",
              "         [ 1.6924e-06,  1.7151e-06,  1.5149e-06,  ...,  1.4176e-06,\n",
              "           1.7898e-06,  1.7749e-06],\n",
              "         [ 2.5697e-05,  2.6035e-05,  2.3003e-05,  ...,  2.1527e-05,\n",
              "           2.7178e-05,  2.6933e-05],\n",
              "         ...,\n",
              "         [ 4.2621e-05,  4.3180e-05,  3.8151e-05,  ...,  3.5706e-05,\n",
              "           4.5078e-05,  4.4666e-05],\n",
              "         [-3.2028e-05, -3.2451e-05, -2.8672e-05,  ..., -2.6831e-05,\n",
              "          -3.3876e-05, -3.3572e-05],\n",
              "         [-6.3030e-05, -6.3860e-05, -5.6424e-05,  ..., -5.2802e-05,\n",
              "          -6.6665e-05, -6.6067e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-10-output-dense-weight': tensor([[-9.0100e-06, -1.0828e-05, -9.8984e-06,  ..., -9.3093e-06,\n",
              "          -9.8125e-06, -9.7546e-06],\n",
              "         [ 3.8540e-06,  4.6315e-06,  4.2341e-06,  ...,  3.9817e-06,\n",
              "           4.1972e-06,  4.1726e-06],\n",
              "         [-4.3965e-06, -5.2867e-06, -4.8301e-06,  ..., -4.5435e-06,\n",
              "          -4.7914e-06, -4.7601e-06],\n",
              "         ...,\n",
              "         [ 4.0676e-06,  4.8870e-06,  4.4684e-06,  ...,  4.2012e-06,\n",
              "           4.4286e-06,  4.4037e-06],\n",
              "         [ 1.3684e-05,  1.6445e-05,  1.5033e-05,  ...,  1.4139e-05,\n",
              "           1.4902e-05,  1.4815e-05],\n",
              "         [ 9.4187e-06,  1.1319e-05,  1.0347e-05,  ...,  9.7315e-06,\n",
              "           1.0257e-05,  1.0197e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-11-attention-output-dense-weight': tensor([[-5.2724e-05, -5.6464e-05, -4.6733e-05,  ..., -5.4190e-05,\n",
              "          -5.3455e-05, -5.3709e-05],\n",
              "         [ 3.4858e-05,  3.7342e-05,  3.0907e-05,  ...,  3.5839e-05,\n",
              "           3.5354e-05,  3.5519e-05],\n",
              "         [ 7.5331e-05,  8.0614e-05,  6.6718e-05,  ...,  7.7362e-05,\n",
              "           7.6307e-05,  7.6687e-05],\n",
              "         ...,\n",
              "         [ 1.0521e-04,  1.1274e-04,  9.3318e-05,  ...,  1.0821e-04,\n",
              "           1.0675e-04,  1.0723e-04],\n",
              "         [ 9.8597e-05,  1.0556e-04,  8.7369e-05,  ...,  1.0131e-04,\n",
              "           9.9932e-05,  1.0041e-04],\n",
              "         [-6.3892e-05, -6.8378e-05, -5.6592e-05,  ..., -6.5620e-05,\n",
              "          -6.4726e-05, -6.5047e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-11-attention-self-key-weight': tensor([[-8.2703e-05, -6.5597e-05, -8.2800e-05,  ..., -7.1153e-05,\n",
              "          -7.0504e-05, -7.1102e-05],\n",
              "         [ 3.0197e-05,  2.3952e-05,  3.0233e-05,  ...,  2.5980e-05,\n",
              "           2.5743e-05,  2.5963e-05],\n",
              "         [-6.0549e-05, -4.8028e-05, -6.0622e-05,  ..., -5.2091e-05,\n",
              "          -5.1618e-05, -5.2066e-05],\n",
              "         ...,\n",
              "         [-1.2075e-05, -9.5751e-06, -1.2092e-05,  ..., -1.0389e-05,\n",
              "          -1.0295e-05, -1.0386e-05],\n",
              "         [-6.0094e-05, -4.7667e-05, -6.0164e-05,  ..., -5.1701e-05,\n",
              "          -5.1230e-05, -5.1669e-05],\n",
              "         [-6.6510e-05, -5.2761e-05, -6.6583e-05,  ..., -5.7220e-05,\n",
              "          -5.6697e-05, -5.7180e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-11-attention-self-query-weight': tensor([[-8.2227e-05, -9.3642e-05, -9.1217e-05,  ..., -9.3520e-05,\n",
              "          -1.0644e-04, -8.5407e-05],\n",
              "         [-5.4605e-05, -6.2184e-05, -6.0576e-05,  ..., -6.2105e-05,\n",
              "          -7.0686e-05, -5.6715e-05],\n",
              "         [-7.5279e-05, -8.5722e-05, -8.3510e-05,  ..., -8.5620e-05,\n",
              "          -9.7449e-05, -7.8173e-05],\n",
              "         ...,\n",
              "         [ 6.4317e-05,  7.3245e-05,  7.1350e-05,  ...,  7.3151e-05,\n",
              "           8.3259e-05,  6.6803e-05],\n",
              "         [-4.3545e-05, -4.9590e-05, -4.8307e-05,  ..., -4.9526e-05,\n",
              "          -5.6370e-05, -4.5228e-05],\n",
              "         [-1.3658e-05, -1.5552e-05, -1.5151e-05,  ..., -1.5534e-05,\n",
              "          -1.7680e-05, -1.4181e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-11-attention-self-value-weight': tensor([[-5.5998e-05, -4.5299e-05, -4.6432e-05,  ..., -3.8975e-05,\n",
              "          -4.2635e-05, -4.8400e-05],\n",
              "         [ 4.3995e-05,  3.5590e-05,  3.6479e-05,  ...,  3.0620e-05,\n",
              "           3.3496e-05,  3.8028e-05],\n",
              "         [-8.3463e-06, -6.7528e-06, -6.9206e-06,  ..., -5.8086e-06,\n",
              "          -6.3542e-06, -7.2183e-06],\n",
              "         ...,\n",
              "         [-3.9133e-05, -3.1642e-05, -3.2456e-05,  ..., -2.7236e-05,\n",
              "          -2.9791e-05, -3.3848e-05],\n",
              "         [ 1.2701e-05,  1.0288e-05,  1.0526e-05,  ...,  8.8398e-06,\n",
              "           9.6723e-06,  1.0970e-05],\n",
              "         [-2.9915e-05, -2.4208e-05, -2.4802e-05,  ..., -2.0820e-05,\n",
              "          -2.2777e-05, -2.5857e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-11-intermediate-dense-weight': tensor([[ 1.7137e-06,  1.2871e-06,  1.9085e-06,  ...,  1.7246e-06,\n",
              "           1.5805e-06,  1.5700e-06],\n",
              "         [-3.8226e-05, -2.8618e-05, -4.2550e-05,  ..., -3.8540e-05,\n",
              "          -3.5218e-05, -3.4901e-05],\n",
              "         [ 3.7283e-05,  2.7917e-05,  4.1501e-05,  ...,  3.7586e-05,\n",
              "           3.4352e-05,  3.4047e-05],\n",
              "         ...,\n",
              "         [ 5.1448e-05,  3.8510e-05,  5.7266e-05,  ...,  5.1877e-05,\n",
              "           4.7398e-05,  4.6965e-05],\n",
              "         [ 3.0673e-05,  2.2963e-05,  3.4142e-05,  ...,  3.0926e-05,\n",
              "           2.8259e-05,  2.8005e-05],\n",
              "         [ 3.8457e-05,  2.8790e-05,  4.2806e-05,  ...,  3.8773e-05,\n",
              "           3.5431e-05,  3.5112e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-11-output-dense-weight': tensor([[-1.1246e-05, -1.1999e-05, -1.3006e-05,  ..., -1.2789e-05,\n",
              "          -1.1853e-05, -1.3543e-05],\n",
              "         [ 2.0061e-05,  2.1405e-05,  2.3201e-05,  ...,  2.2813e-05,\n",
              "           2.1144e-05,  2.4158e-05],\n",
              "         [ 1.2736e-05,  1.3591e-05,  1.4733e-05,  ...,  1.4487e-05,\n",
              "           1.3428e-05,  1.5336e-05],\n",
              "         ...,\n",
              "         [-6.2876e-06, -6.7091e-06, -7.2724e-06,  ..., -7.1510e-06,\n",
              "          -6.6280e-06, -7.5715e-06],\n",
              "         [ 1.2548e-05,  1.3388e-05,  1.4511e-05,  ...,  1.4269e-05,\n",
              "           1.3225e-05,  1.5111e-05],\n",
              "         [ 1.3244e-05,  1.4131e-05,  1.5317e-05,  ...,  1.5061e-05,\n",
              "           1.3959e-05,  1.5949e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-2-attention-output-dense-weight': tensor([[-1.5789e-05, -1.6878e-05, -1.5943e-05,  ..., -1.4816e-05,\n",
              "          -1.6787e-05, -1.6795e-05],\n",
              "         [-4.1223e-06, -4.4197e-06, -4.1681e-06,  ..., -3.8785e-06,\n",
              "          -4.3801e-06, -4.4016e-06],\n",
              "         [ 7.1615e-05,  7.6553e-05,  7.2311e-05,  ...,  6.7199e-05,\n",
              "           7.6129e-05,  7.6176e-05],\n",
              "         ...,\n",
              "         [ 8.2432e-05,  8.8071e-05,  8.3205e-05,  ...,  7.7317e-05,\n",
              "           8.7622e-05,  8.7630e-05],\n",
              "         [-1.9847e-05, -2.1229e-05, -2.0054e-05,  ..., -1.8630e-05,\n",
              "          -2.1133e-05, -2.1115e-05],\n",
              "         [-5.4239e-05, -5.7974e-05, -5.4765e-05,  ..., -5.0889e-05,\n",
              "          -5.7673e-05, -5.7681e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-2-attention-self-key-weight': tensor([[ 3.1432e-05,  3.1588e-05,  2.6058e-05,  ...,  3.0474e-05,\n",
              "           3.5158e-05,  2.7383e-05],\n",
              "         [-5.6975e-05, -5.7257e-05, -4.7234e-05,  ..., -5.5237e-05,\n",
              "          -6.3728e-05, -4.9635e-05],\n",
              "         [-6.7464e-05, -6.7799e-05, -5.5932e-05,  ..., -6.5406e-05,\n",
              "          -7.5464e-05, -5.8774e-05],\n",
              "         ...,\n",
              "         [ 7.0351e-05,  7.0699e-05,  5.8324e-05,  ...,  6.8206e-05,\n",
              "           7.8691e-05,  6.1289e-05],\n",
              "         [-6.1494e-06, -6.1830e-06, -5.1014e-06,  ..., -5.9642e-06,\n",
              "          -6.8810e-06, -5.3624e-06],\n",
              "         [ 1.6419e-05,  1.6501e-05,  1.3612e-05,  ...,  1.5918e-05,\n",
              "           1.8366e-05,  1.4304e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-2-attention-self-query-weight': tensor([[ 7.5410e-05,  7.8532e-05,  7.7353e-05,  ...,  8.0732e-05,\n",
              "           7.3966e-05,  8.7184e-05],\n",
              "         [-4.2795e-05, -4.4566e-05, -4.3897e-05,  ..., -4.5815e-05,\n",
              "          -4.1976e-05, -4.9475e-05],\n",
              "         [ 3.3122e-05,  3.4493e-05,  3.3975e-05,  ...,  3.5459e-05,\n",
              "           3.2487e-05,  3.8293e-05],\n",
              "         ...,\n",
              "         [ 4.9120e-05,  5.1172e-05,  5.0404e-05,  ...,  5.2601e-05,\n",
              "           4.8191e-05,  5.6824e-05],\n",
              "         [ 3.2548e-05,  3.3893e-05,  3.3384e-05,  ...,  3.4843e-05,\n",
              "           3.1924e-05,  3.7623e-05],\n",
              "         [-6.6428e-05, -6.9176e-05, -6.8137e-05,  ..., -7.1113e-05,\n",
              "          -6.5154e-05, -7.6796e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-2-attention-self-value-weight': tensor([[ 4.6071e-05,  5.4091e-05,  6.5397e-05,  ...,  5.9475e-05,\n",
              "           4.5577e-05,  5.7773e-05],\n",
              "         [-3.9720e-05, -4.6626e-05, -5.6380e-05,  ..., -5.1291e-05,\n",
              "          -3.9296e-05, -4.9840e-05],\n",
              "         [-6.3646e-05, -7.4731e-05, -9.0347e-05,  ..., -8.2168e-05,\n",
              "          -6.2966e-05, -7.9822e-05],\n",
              "         ...,\n",
              "         [ 7.1074e-06,  8.3447e-06,  1.0089e-05,  ...,  9.1748e-06,\n",
              "           7.0346e-06,  8.9039e-06],\n",
              "         [-4.1154e-05, -4.8316e-05, -5.8416e-05,  ..., -5.3129e-05,\n",
              "          -4.0713e-05, -5.1610e-05],\n",
              "         [-2.3788e-05, -2.7926e-05, -3.3763e-05,  ..., -3.0707e-05,\n",
              "          -2.3529e-05, -2.9832e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-2-intermediate-dense-weight': tensor([[ 1.9513e-05,  2.0194e-05,  1.8153e-05,  ...,  2.2637e-05,\n",
              "           2.2330e-05,  2.3084e-05],\n",
              "         [-4.1389e-05, -4.2844e-05, -3.8507e-05,  ..., -4.8016e-05,\n",
              "          -4.7364e-05, -4.8974e-05],\n",
              "         [-4.6499e-05, -4.8116e-05, -4.3245e-05,  ..., -5.3926e-05,\n",
              "          -5.3199e-05, -5.4993e-05],\n",
              "         ...,\n",
              "         [ 7.2181e-05,  7.4703e-05,  6.7149e-05,  ...,  8.3738e-05,\n",
              "           8.2603e-05,  8.5393e-05],\n",
              "         [-8.8571e-06, -9.1644e-06, -8.2369e-06,  ..., -1.0274e-05,\n",
              "          -1.0133e-05, -1.0478e-05],\n",
              "         [ 8.0327e-05,  8.3200e-05,  7.4784e-05,  ...,  9.3249e-05,\n",
              "           9.1980e-05,  9.5225e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-2-output-dense-weight': tensor([[-9.3582e-06, -9.9075e-06, -9.8504e-06,  ..., -1.0670e-05,\n",
              "          -8.1143e-06, -8.7232e-06],\n",
              "         [ 6.3957e-07,  6.7832e-07,  6.7486e-07,  ...,  7.3001e-07,\n",
              "           5.5673e-07,  5.9854e-07],\n",
              "         [-2.3644e-06, -2.5031e-06, -2.4887e-06,  ..., -2.6965e-06,\n",
              "          -2.0500e-06, -2.1958e-06],\n",
              "         ...,\n",
              "         [ 1.4037e-05,  1.4859e-05,  1.4776e-05,  ...,  1.6005e-05,\n",
              "           1.2172e-05,  1.3088e-05],\n",
              "         [ 3.1716e-07,  3.3474e-07,  3.3438e-07,  ...,  3.6159e-07,\n",
              "           2.7500e-07,  3.0079e-07],\n",
              "         [ 5.7498e-06,  6.0863e-06,  6.0526e-06,  ...,  6.5560e-06,\n",
              "           4.9858e-06,  5.3612e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-3-attention-output-dense-weight': tensor([[ 3.6162e-05,  3.3028e-05,  4.1269e-05,  ...,  3.3911e-05,\n",
              "           3.2914e-05,  3.2291e-05],\n",
              "         [ 2.3999e-05,  2.1911e-05,  2.7393e-05,  ...,  2.2509e-05,\n",
              "           2.1845e-05,  2.1433e-05],\n",
              "         [-6.0003e-06, -5.4797e-06, -6.8521e-06,  ..., -5.6251e-06,\n",
              "          -5.4622e-06, -5.3654e-06],\n",
              "         ...,\n",
              "         [ 6.3457e-05,  5.7959e-05,  7.2432e-05,  ...,  5.9497e-05,\n",
              "           5.7760e-05,  5.6691e-05],\n",
              "         [-8.4865e-05, -7.7518e-05, -9.6878e-05,  ..., -7.9566e-05,\n",
              "          -7.7247e-05, -7.5828e-05],\n",
              "         [ 1.7338e-05,  1.5844e-05,  1.9793e-05,  ...,  1.6254e-05,\n",
              "           1.5781e-05,  1.5491e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-3-attention-self-key-weight': tensor([[ 1.4561e-04,  1.2352e-04,  1.6201e-04,  ...,  1.5175e-04,\n",
              "           1.4261e-04,  1.6213e-04],\n",
              "         [ 1.8634e-05,  1.5802e-05,  2.0717e-05,  ...,  1.9405e-05,\n",
              "           1.8234e-05,  2.0701e-05],\n",
              "         [ 1.2029e-05,  1.0205e-05,  1.3384e-05,  ...,  1.2535e-05,\n",
              "           1.1780e-05,  1.3390e-05],\n",
              "         ...,\n",
              "         [-1.7462e-05, -1.4815e-05, -1.9433e-05,  ..., -1.8202e-05,\n",
              "          -1.7106e-05, -1.9453e-05],\n",
              "         [-9.2806e-06, -7.8716e-06, -1.0319e-05,  ..., -9.6645e-06,\n",
              "          -9.0821e-06, -1.0312e-05],\n",
              "         [-8.9991e-05, -7.6348e-05, -1.0015e-04,  ..., -9.3803e-05,\n",
              "          -8.8155e-05, -1.0025e-04]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-3-attention-self-query-weight': tensor([[ 1.2192e-05,  1.3710e-05,  1.4298e-05,  ...,  1.4580e-05,\n",
              "           1.4675e-05,  1.5582e-05],\n",
              "         [ 5.3841e-05,  6.0585e-05,  6.3188e-05,  ...,  6.4427e-05,\n",
              "           6.4849e-05,  6.8937e-05],\n",
              "         [ 2.3218e-05,  2.6132e-05,  2.7255e-05,  ...,  2.7789e-05,\n",
              "           2.7971e-05,  2.9746e-05],\n",
              "         ...,\n",
              "         [ 2.9522e-05,  3.3206e-05,  3.4636e-05,  ...,  3.5319e-05,\n",
              "           3.5550e-05,  3.7772e-05],\n",
              "         [ 6.6352e-05,  7.4636e-05,  7.7839e-05,  ...,  7.9371e-05,\n",
              "           7.9887e-05,  8.4868e-05],\n",
              "         [-1.2809e-05, -1.4408e-05, -1.5027e-05,  ..., -1.5323e-05,\n",
              "          -1.5422e-05, -1.6384e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-3-attention-self-value-weight': tensor([[-4.7172e-05, -5.1301e-05, -4.5165e-05,  ..., -4.6750e-05,\n",
              "          -4.5028e-05, -4.3761e-05],\n",
              "         [ 1.2239e-05,  1.3320e-05,  1.1724e-05,  ...,  1.2133e-05,\n",
              "           1.1686e-05,  1.1372e-05],\n",
              "         [ 9.7050e-05,  1.0555e-04,  9.2919e-05,  ...,  9.6181e-05,\n",
              "           9.2636e-05,  9.0034e-05],\n",
              "         ...,\n",
              "         [-5.2205e-05, -5.6774e-05, -4.9980e-05,  ..., -5.1737e-05,\n",
              "          -4.9831e-05, -4.8431e-05],\n",
              "         [ 5.4591e-05,  5.9382e-05,  5.2274e-05,  ...,  5.4104e-05,\n",
              "           5.2108e-05,  5.0653e-05],\n",
              "         [ 3.2351e-05,  3.5208e-05,  3.0987e-05,  ...,  3.2063e-05,\n",
              "           3.0879e-05,  3.0023e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-3-intermediate-dense-weight': tensor([[ 1.5764e-05,  1.8165e-05,  1.5591e-05,  ...,  1.6165e-05,\n",
              "           1.3789e-05,  1.7513e-05],\n",
              "         [ 1.0227e-04,  1.1771e-04,  1.0104e-04,  ...,  1.0478e-04,\n",
              "           8.9349e-05,  1.1333e-04],\n",
              "         [ 9.9753e-05,  1.1481e-04,  9.8551e-05,  ...,  1.0220e-04,\n",
              "           8.7151e-05,  1.1054e-04],\n",
              "         ...,\n",
              "         [-1.9475e-05, -2.2419e-05, -1.9239e-05,  ..., -1.9952e-05,\n",
              "          -1.7014e-05, -2.1573e-05],\n",
              "         [ 2.8766e-05,  3.3111e-05,  2.8417e-05,  ...,  2.9469e-05,\n",
              "           2.5131e-05,  3.1874e-05],\n",
              "         [-4.6133e-05, -5.3099e-05, -4.5578e-05,  ..., -4.7266e-05,\n",
              "          -4.0306e-05, -5.1124e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-3-output-dense-weight': tensor([[-8.1171e-06, -7.0639e-06, -7.6725e-06,  ..., -7.6413e-06,\n",
              "          -7.5813e-06, -7.0533e-06],\n",
              "         [-5.2797e-06, -4.5927e-06, -4.9913e-06,  ..., -4.9718e-06,\n",
              "          -4.9313e-06, -4.5887e-06],\n",
              "         [-1.2615e-06, -1.0943e-06, -1.1896e-06,  ..., -1.1866e-06,\n",
              "          -1.1760e-06, -1.0953e-06],\n",
              "         ...,\n",
              "         [-1.0190e-05, -8.8691e-06, -9.6356e-06,  ..., -9.5944e-06,\n",
              "          -9.5201e-06, -8.8553e-06],\n",
              "         [ 2.8844e-06,  2.5097e-06,  2.7241e-06,  ...,  2.7138e-06,\n",
              "           2.6928e-06,  2.5054e-06],\n",
              "         [ 1.0810e-05,  9.4081e-06,  1.0218e-05,  ...,  1.0176e-05,\n",
              "           1.0097e-05,  9.3934e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-4-attention-output-dense-weight': tensor([[-1.9972e-05, -1.7356e-05, -1.7246e-05,  ..., -1.7187e-05,\n",
              "          -1.7544e-05, -1.7010e-05],\n",
              "         [ 3.5913e-05,  3.1202e-05,  3.1011e-05,  ...,  3.0897e-05,\n",
              "           3.1556e-05,  3.0578e-05],\n",
              "         [-4.9580e-05, -4.3084e-05, -4.2813e-05,  ..., -4.2650e-05,\n",
              "          -4.3557e-05, -4.2215e-05],\n",
              "         ...,\n",
              "         [ 8.2641e-05,  7.1797e-05,  7.1363e-05,  ...,  7.1108e-05,\n",
              "           7.2595e-05,  7.0403e-05],\n",
              "         [-8.3661e-05, -7.2687e-05, -7.2243e-05,  ..., -7.1984e-05,\n",
              "          -7.3493e-05, -7.1263e-05],\n",
              "         [ 2.4413e-06,  2.1144e-06,  2.1087e-06,  ...,  2.1125e-06,\n",
              "           2.1472e-06,  2.0910e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-4-attention-self-key-weight': tensor([[ 3.2557e-05,  3.4458e-05,  3.4507e-05,  ...,  3.7610e-05,\n",
              "           3.2644e-05,  3.8103e-05],\n",
              "         [ 3.4939e-05,  3.6979e-05,  3.7031e-05,  ...,  4.0361e-05,\n",
              "           3.5033e-05,  4.0890e-05],\n",
              "         [ 5.2714e-05,  5.5792e-05,  5.5869e-05,  ...,  6.0893e-05,\n",
              "           5.2853e-05,  6.1686e-05],\n",
              "         ...,\n",
              "         [ 1.4500e-05,  1.5347e-05,  1.5368e-05,  ...,  1.6751e-05,\n",
              "           1.4538e-05,  1.6968e-05],\n",
              "         [-2.9908e-05, -3.1653e-05, -3.1700e-05,  ..., -3.4549e-05,\n",
              "          -2.9985e-05, -3.4998e-05],\n",
              "         [-1.1758e-05, -1.2438e-05, -1.2452e-05,  ..., -1.3574e-05,\n",
              "          -1.1784e-05, -1.3749e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-4-attention-self-query-weight': tensor([[ 9.6186e-06,  8.6620e-06,  9.1514e-06,  ...,  9.4382e-06,\n",
              "           1.0036e-05,  8.8457e-06],\n",
              "         [-3.0931e-05, -2.7847e-05, -2.9429e-05,  ..., -3.0356e-05,\n",
              "          -3.2275e-05, -2.8453e-05],\n",
              "         [-7.2600e-05, -6.5362e-05, -6.9073e-05,  ..., -7.1245e-05,\n",
              "          -7.5752e-05, -6.6775e-05],\n",
              "         ...,\n",
              "         [ 5.6211e-05,  5.0599e-05,  5.3475e-05,  ...,  5.5165e-05,\n",
              "           5.8651e-05,  5.1708e-05],\n",
              "         [ 6.3913e-05,  5.7540e-05,  6.0810e-05,  ...,  6.2717e-05,\n",
              "           6.6687e-05,  5.8783e-05],\n",
              "         [ 4.6887e-05,  4.2211e-05,  4.4606e-05,  ...,  4.6010e-05,\n",
              "           4.8921e-05,  4.3123e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-4-attention-self-value-weight': tensor([[ 7.1293e-07,  7.4529e-07,  7.4686e-07,  ...,  8.4189e-07,\n",
              "           7.0044e-07,  7.0867e-07],\n",
              "         [ 2.6346e-05,  2.7820e-05,  2.8221e-05,  ...,  3.1433e-05,\n",
              "           2.6103e-05,  2.6368e-05],\n",
              "         [ 5.6927e-05,  6.0122e-05,  6.0974e-05,  ...,  6.7910e-05,\n",
              "           5.6392e-05,  5.6971e-05],\n",
              "         ...,\n",
              "         [-9.0935e-05, -9.6040e-05, -9.7397e-05,  ..., -1.0848e-04,\n",
              "          -9.0080e-05, -9.1000e-05],\n",
              "         [-5.3707e-05, -5.6721e-05, -5.7528e-05,  ..., -6.4069e-05,\n",
              "          -5.3203e-05, -5.3749e-05],\n",
              "         [ 1.7504e-05,  1.8495e-05,  1.8794e-05,  ...,  2.0899e-05,\n",
              "           1.7353e-05,  1.7567e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-4-intermediate-dense-weight': tensor([[-2.2649e-05, -2.3973e-05, -2.2553e-05,  ..., -2.3566e-05,\n",
              "          -1.7995e-05, -2.2015e-05],\n",
              "         [ 6.7496e-05,  7.1442e-05,  6.7208e-05,  ...,  7.0228e-05,\n",
              "           5.3627e-05,  6.5603e-05],\n",
              "         [ 2.4959e-05,  2.6418e-05,  2.4853e-05,  ...,  2.5970e-05,\n",
              "           1.9830e-05,  2.4260e-05],\n",
              "         ...,\n",
              "         [ 2.9556e-05,  3.1282e-05,  2.9431e-05,  ...,  3.0753e-05,\n",
              "           2.3486e-05,  2.8735e-05],\n",
              "         [-4.1587e-05, -4.4016e-05, -4.1410e-05,  ..., -4.3270e-05,\n",
              "          -3.3041e-05, -4.0422e-05],\n",
              "         [ 8.7059e-05,  9.2147e-05,  8.6688e-05,  ...,  9.0582e-05,\n",
              "           6.9168e-05,  8.4617e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-4-output-dense-weight': tensor([[-8.3008e-06, -8.6929e-06, -7.3882e-06,  ..., -8.4292e-06,\n",
              "          -7.8034e-06, -7.6689e-06],\n",
              "         [ 1.1551e-05,  1.2095e-05,  1.0281e-05,  ...,  1.1729e-05,\n",
              "           1.0860e-05,  1.0672e-05],\n",
              "         [-9.8068e-06, -1.0269e-05, -8.7289e-06,  ..., -9.9576e-06,\n",
              "          -9.2200e-06, -9.0601e-06],\n",
              "         ...,\n",
              "         [-1.8076e-05, -1.8929e-05, -1.6089e-05,  ..., -1.8355e-05,\n",
              "          -1.6994e-05, -1.6700e-05],\n",
              "         [ 6.4368e-06,  6.7399e-06,  5.7291e-06,  ...,  6.5358e-06,\n",
              "           6.0515e-06,  5.9465e-06],\n",
              "         [ 6.9013e-06,  7.2262e-06,  6.1426e-06,  ...,  7.0075e-06,\n",
              "           6.4882e-06,  6.3757e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-5-attention-output-dense-weight': tensor([[ 6.5968e-06,  6.5043e-06,  7.0634e-06,  ...,  5.9130e-06,\n",
              "           5.9449e-06,  6.7281e-06],\n",
              "         [-3.1341e-05, -3.0833e-05, -3.3509e-05,  ..., -2.8060e-05,\n",
              "          -2.8185e-05, -3.1899e-05],\n",
              "         [-2.1296e-05, -2.0941e-05, -2.2769e-05,  ..., -1.9059e-05,\n",
              "          -1.9143e-05, -2.1677e-05],\n",
              "         ...,\n",
              "         [-5.0362e-05, -4.9518e-05, -5.3839e-05,  ..., -4.5071e-05,\n",
              "          -4.5267e-05, -5.1249e-05],\n",
              "         [ 1.2277e-04,  1.2074e-04,  1.3125e-04,  ...,  1.0989e-04,\n",
              "           1.1037e-04,  1.2495e-04],\n",
              "         [-7.5057e-06, -7.3833e-06, -8.0234e-06,  ..., -6.7178e-06,\n",
              "          -6.7492e-06, -7.6367e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-5-attention-self-key-weight': tensor([[-8.6558e-05, -8.3564e-05, -9.0784e-05,  ..., -9.7877e-05,\n",
              "          -8.4645e-05, -8.7993e-05],\n",
              "         [-9.4598e-05, -9.1327e-05, -9.9218e-05,  ..., -1.0697e-04,\n",
              "          -9.2507e-05, -9.6168e-05],\n",
              "         [ 5.0334e-05,  4.8589e-05,  5.2787e-05,  ...,  5.6910e-05,\n",
              "           4.9219e-05,  5.1169e-05],\n",
              "         ...,\n",
              "         [-1.5697e-05, -1.5154e-05, -1.6464e-05,  ..., -1.7750e-05,\n",
              "          -1.5351e-05, -1.5960e-05],\n",
              "         [-2.3090e-05, -2.2289e-05, -2.4212e-05,  ..., -2.6104e-05,\n",
              "          -2.2574e-05, -2.3456e-05],\n",
              "         [-1.1959e-04, -1.1545e-04, -1.2542e-04,  ..., -1.3522e-04,\n",
              "          -1.1694e-04, -1.2157e-04]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-5-attention-self-query-weight': tensor([[ 1.2953e-04,  1.3004e-04,  1.2739e-04,  ...,  1.0964e-04,\n",
              "           1.2054e-04,  1.2289e-04],\n",
              "         [-1.8291e-05, -1.8361e-05, -1.7989e-05,  ..., -1.5482e-05,\n",
              "          -1.7022e-05, -1.7357e-05],\n",
              "         [ 5.7587e-07,  5.7829e-07,  5.6493e-07,  ...,  4.8725e-07,\n",
              "           5.3610e-07,  5.4454e-07],\n",
              "         ...,\n",
              "         [ 1.2723e-05,  1.2772e-05,  1.2509e-05,  ...,  1.0769e-05,\n",
              "           1.1840e-05,  1.2067e-05],\n",
              "         [-2.9099e-05, -2.9213e-05, -2.8618e-05,  ..., -2.4630e-05,\n",
              "          -2.7077e-05, -2.7607e-05],\n",
              "         [ 1.9466e-05,  1.9542e-05,  1.9144e-05,  ...,  1.6476e-05,\n",
              "           1.8114e-05,  1.8466e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-5-attention-self-value-weight': tensor([[-8.1218e-06, -9.2947e-06, -8.3162e-06,  ..., -9.2663e-06,\n",
              "          -8.8873e-06, -9.0836e-06],\n",
              "         [-4.1572e-05, -4.7567e-05, -4.2571e-05,  ..., -4.7428e-05,\n",
              "          -4.5482e-05, -4.6489e-05],\n",
              "         [ 2.3919e-05,  2.7371e-05,  2.4501e-05,  ...,  2.7296e-05,\n",
              "           2.6175e-05,  2.6760e-05],\n",
              "         ...,\n",
              "         [ 1.5824e-05,  1.8117e-05,  1.6200e-05,  ...,  1.8062e-05,\n",
              "           1.7334e-05,  1.7707e-05],\n",
              "         [ 2.0387e-05,  2.3337e-05,  2.0877e-05,  ...,  2.3268e-05,\n",
              "           2.2321e-05,  2.2811e-05],\n",
              "         [ 2.5664e-05,  2.9370e-05,  2.6286e-05,  ...,  2.9288e-05,\n",
              "           2.8087e-05,  2.8712e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-5-intermediate-dense-weight': tensor([[ 9.4658e-05,  8.9052e-05,  8.0994e-05,  ...,  1.0024e-04,\n",
              "           8.7160e-05,  8.7493e-05],\n",
              "         [ 7.2491e-05,  6.8200e-05,  6.2032e-05,  ...,  7.6770e-05,\n",
              "           6.6751e-05,  6.7013e-05],\n",
              "         [ 7.0339e-05,  6.6175e-05,  6.0190e-05,  ...,  7.4490e-05,\n",
              "           6.4769e-05,  6.5023e-05],\n",
              "         ...,\n",
              "         [-9.5716e-05, -9.0044e-05, -8.1909e-05,  ..., -1.0136e-04,\n",
              "          -8.8128e-05, -8.8472e-05],\n",
              "         [-7.5001e-05, -7.0562e-05, -6.4180e-05,  ..., -7.9428e-05,\n",
              "          -6.9063e-05, -6.9335e-05],\n",
              "         [-3.9977e-05, -3.7611e-05, -3.4209e-05,  ..., -4.2337e-05,\n",
              "          -3.6812e-05, -3.6955e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-5-output-dense-weight': tensor([[ 5.8845e-06,  7.0019e-06,  5.6759e-06,  ...,  5.3089e-06,\n",
              "           6.7472e-06,  6.2804e-06],\n",
              "         [-3.5975e-07, -4.2938e-07, -3.4740e-07,  ..., -3.2541e-07,\n",
              "          -4.1309e-07, -3.8479e-07],\n",
              "         [ 1.0426e-05,  1.2399e-05,  1.0054e-05,  ...,  9.4021e-06,\n",
              "           1.1953e-05,  1.1122e-05],\n",
              "         ...,\n",
              "         [ 7.3389e-07,  8.7464e-07,  7.0863e-07,  ...,  6.6310e-07,\n",
              "           8.4223e-07,  7.8389e-07],\n",
              "         [ 1.0608e-05,  1.2615e-05,  1.0229e-05,  ...,  9.5659e-06,\n",
              "           1.2162e-05,  1.1316e-05],\n",
              "         [-9.2486e-06, -1.0998e-05, -8.9190e-06,  ..., -8.3391e-06,\n",
              "          -1.0601e-05, -9.8676e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-6-attention-output-dense-weight': tensor([[-2.3511e-05, -2.3180e-05, -2.3520e-05,  ..., -2.5657e-05,\n",
              "          -2.2103e-05, -2.5326e-05],\n",
              "         [ 1.7269e-06,  1.7039e-06,  1.7302e-06,  ...,  1.8816e-06,\n",
              "           1.6216e-06,  1.8570e-06],\n",
              "         [-4.7787e-05, -4.7124e-05, -4.7826e-05,  ..., -5.2136e-05,\n",
              "          -4.4911e-05, -5.1457e-05],\n",
              "         ...,\n",
              "         [-1.1707e-04, -1.1541e-04, -1.1709e-04,  ..., -1.2771e-04,\n",
              "          -1.0999e-04, -1.2604e-04],\n",
              "         [-4.0531e-05, -3.9959e-05, -4.0542e-05,  ..., -4.4218e-05,\n",
              "          -3.8080e-05, -4.3635e-05],\n",
              "         [ 2.5883e-05,  2.5528e-05,  2.5915e-05,  ...,  2.8240e-05,\n",
              "           2.4332e-05,  2.7867e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-6-attention-self-key-weight': tensor([[ 6.0827e-05,  6.0159e-05,  6.3489e-05,  ...,  6.8857e-05,\n",
              "           5.6612e-05,  5.9688e-05],\n",
              "         [ 7.6116e-08,  7.4288e-08,  7.9024e-08,  ...,  8.5979e-08,\n",
              "           6.9842e-08,  7.3366e-08],\n",
              "         [-5.6434e-05, -5.5809e-05, -5.8900e-05,  ..., -6.3878e-05,\n",
              "          -5.2514e-05, -5.5363e-05],\n",
              "         ...,\n",
              "         [ 8.1740e-05,  8.0844e-05,  8.5318e-05,  ...,  9.2531e-05,\n",
              "           7.6077e-05,  8.0212e-05],\n",
              "         [-1.3128e-05, -1.2984e-05, -1.3703e-05,  ..., -1.4861e-05,\n",
              "          -1.2219e-05, -1.2883e-05],\n",
              "         [ 3.7329e-05,  3.6923e-05,  3.8960e-05,  ...,  4.2262e-05,\n",
              "           3.4751e-05,  3.6634e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-6-attention-self-query-weight': tensor([[ 4.2578e-05,  3.9039e-05,  3.6246e-05,  ...,  4.0918e-05,\n",
              "           5.0622e-05,  4.4735e-05],\n",
              "         [-1.4015e-05, -1.2851e-05, -1.1931e-05,  ..., -1.3469e-05,\n",
              "          -1.6663e-05, -1.4725e-05],\n",
              "         [ 1.1843e-05,  1.0859e-05,  1.0082e-05,  ...,  1.1382e-05,\n",
              "           1.4082e-05,  1.2445e-05],\n",
              "         ...,\n",
              "         [-6.4749e-06, -5.9356e-06, -5.5111e-06,  ..., -6.2209e-06,\n",
              "          -7.6964e-06, -6.7999e-06],\n",
              "         [-1.8820e-05, -1.7256e-05, -1.6021e-05,  ..., -1.8086e-05,\n",
              "          -2.2376e-05, -1.9773e-05],\n",
              "         [ 3.3477e-05,  3.0695e-05,  2.8499e-05,  ...,  3.2172e-05,\n",
              "           3.9801e-05,  3.5171e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-6-attention-self-value-weight': tensor([[ 9.6490e-05,  8.0041e-05,  8.5853e-05,  ...,  8.5293e-05,\n",
              "           8.6313e-05,  1.0070e-04],\n",
              "         [ 1.0092e-04,  8.3722e-05,  8.9768e-05,  ...,  8.9222e-05,\n",
              "           9.0302e-05,  1.0532e-04],\n",
              "         [-9.3219e-05, -7.7333e-05, -8.2938e-05,  ..., -8.2407e-05,\n",
              "          -8.3398e-05, -9.7294e-05],\n",
              "         ...,\n",
              "         [ 7.5337e-05,  6.2498e-05,  6.7027e-05,  ...,  6.6595e-05,\n",
              "           6.7397e-05,  7.8620e-05],\n",
              "         [ 2.5873e-06,  2.1311e-06,  2.3119e-06,  ...,  2.2536e-06,\n",
              "           2.2775e-06,  2.6704e-06],\n",
              "         [-1.3072e-04, -1.0844e-04, -1.1631e-04,  ..., -1.1554e-04,\n",
              "          -1.1693e-04, -1.3642e-04]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-6-intermediate-dense-weight': tensor([[-7.2834e-06, -7.3696e-06, -7.1631e-06,  ..., -9.2721e-06,\n",
              "          -7.9839e-06, -7.3183e-06],\n",
              "         [ 5.5959e-06,  5.6551e-06,  5.5018e-06,  ...,  7.1142e-06,\n",
              "           6.1254e-06,  5.6066e-06],\n",
              "         [ 1.8630e-05,  1.8825e-05,  1.8317e-05,  ...,  2.3682e-05,\n",
              "           2.0390e-05,  1.8661e-05],\n",
              "         ...,\n",
              "         [-1.2319e-04, -1.2449e-04, -1.2113e-04,  ..., -1.5661e-04,\n",
              "          -1.3485e-04, -1.2343e-04],\n",
              "         [ 4.8619e-05,  4.9131e-05,  4.7805e-05,  ...,  6.1809e-05,\n",
              "           5.3219e-05,  4.8711e-05],\n",
              "         [ 3.2304e-05,  3.2643e-05,  3.1764e-05,  ...,  4.1066e-05,\n",
              "           3.5360e-05,  3.2365e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-6-output-dense-weight': tensor([[ 6.1819e-06,  6.2312e-06,  6.4965e-06,  ...,  5.8535e-06,\n",
              "           6.5830e-06,  6.3130e-06],\n",
              "         [ 9.6787e-06,  9.7582e-06,  1.0175e-05,  ...,  9.1707e-06,\n",
              "           1.0308e-05,  9.8871e-06],\n",
              "         [-8.3529e-07, -8.4210e-07, -8.7879e-07,  ..., -7.9168e-07,\n",
              "          -8.8936e-07, -8.5317e-07],\n",
              "         ...,\n",
              "         [-2.9635e-06, -2.9878e-06, -3.1154e-06,  ..., -2.8077e-06,\n",
              "          -3.1563e-06, -3.0272e-06],\n",
              "         [-8.1662e-06, -8.2323e-06, -8.5831e-06,  ..., -7.7348e-06,\n",
              "          -8.6968e-06, -8.3407e-06],\n",
              "         [ 9.7207e-06,  9.8048e-06,  1.0220e-05,  ...,  9.2138e-06,\n",
              "           1.0359e-05,  9.9360e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-7-attention-output-dense-weight': tensor([[ 9.3100e-06,  1.0300e-05,  9.6977e-06,  ...,  9.2048e-06,\n",
              "           1.0469e-05,  1.0255e-05],\n",
              "         [ 8.9399e-05,  9.8896e-05,  9.3129e-05,  ...,  8.8354e-05,\n",
              "           1.0063e-04,  9.8480e-05],\n",
              "         [ 6.0265e-05,  6.6666e-05,  6.2778e-05,  ...,  5.9561e-05,\n",
              "           6.7825e-05,  6.6386e-05],\n",
              "         ...,\n",
              "         [-3.5805e-05, -3.9608e-05, -3.7298e-05,  ..., -3.5388e-05,\n",
              "          -4.0298e-05, -3.9441e-05],\n",
              "         [-9.1500e-05, -1.0122e-04, -9.5317e-05,  ..., -9.0434e-05,\n",
              "          -1.0298e-04, -1.0079e-04],\n",
              "         [ 3.5914e-05,  3.9721e-05,  3.7418e-05,  ...,  3.5520e-05,\n",
              "           4.0426e-05,  3.9551e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-7-attention-self-key-weight': tensor([[-1.6741e-04, -1.8880e-04, -1.8025e-04,  ..., -1.8119e-04,\n",
              "          -1.6531e-04, -1.5590e-04],\n",
              "         [ 7.8631e-05,  8.8682e-05,  8.4665e-05,  ...,  8.5109e-05,\n",
              "           7.7649e-05,  7.3233e-05],\n",
              "         [-2.7245e-06, -3.0595e-06, -2.9249e-06,  ..., -2.9341e-06,\n",
              "          -2.6795e-06, -2.5202e-06],\n",
              "         ...,\n",
              "         [-4.5905e-05, -5.1769e-05, -4.9425e-05,  ..., -4.9682e-05,\n",
              "          -4.5328e-05, -4.2748e-05],\n",
              "         [-3.6904e-06, -4.1598e-06, -3.9725e-06,  ..., -3.9928e-06,\n",
              "          -3.6425e-06, -3.4332e-06],\n",
              "         [ 1.2824e-05,  1.4458e-05,  1.3806e-05,  ...,  1.3876e-05,\n",
              "           1.2660e-05,  1.1938e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-7-attention-self-query-weight': tensor([[ 2.8933e-05,  2.7936e-05,  2.2834e-05,  ...,  2.2705e-05,\n",
              "           2.4798e-05,  2.7480e-05],\n",
              "         [-2.2170e-05, -2.1407e-05, -1.7496e-05,  ..., -1.7397e-05,\n",
              "          -1.8999e-05, -2.1048e-05],\n",
              "         [-6.4937e-05, -6.2703e-05, -5.1255e-05,  ..., -5.0954e-05,\n",
              "          -5.5649e-05, -6.1656e-05],\n",
              "         ...,\n",
              "         [ 3.1104e-05,  3.0033e-05,  2.4550e-05,  ...,  2.4405e-05,\n",
              "           2.6655e-05,  2.9532e-05],\n",
              "         [-2.0676e-05, -1.9969e-05, -1.6326e-05,  ..., -1.6230e-05,\n",
              "          -1.7727e-05, -1.9655e-05],\n",
              "         [ 1.4441e-06,  1.3948e-06,  1.1402e-06,  ...,  1.1337e-06,\n",
              "           1.2382e-06,  1.3727e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-7-attention-self-value-weight': tensor([[-5.2629e-06, -6.3377e-06, -6.2826e-06,  ..., -7.0554e-06,\n",
              "          -6.3196e-06, -6.5858e-06],\n",
              "         [-1.2326e-04, -1.4844e-04, -1.4714e-04,  ..., -1.6525e-04,\n",
              "          -1.4801e-04, -1.5425e-04],\n",
              "         [-6.1818e-05, -7.4445e-05, -7.3794e-05,  ..., -8.2877e-05,\n",
              "          -7.4231e-05, -7.7358e-05],\n",
              "         ...,\n",
              "         [ 4.3020e-05,  5.1796e-05,  5.1345e-05,  ...,  5.7662e-05,\n",
              "           5.1642e-05,  5.3813e-05],\n",
              "         [ 3.1533e-05,  3.7991e-05,  3.7649e-05,  ...,  4.2287e-05,\n",
              "           3.7878e-05,  3.9481e-05],\n",
              "         [ 1.8710e-05,  2.2493e-05,  2.2304e-05,  ...,  2.5033e-05,\n",
              "           2.2429e-05,  2.3333e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-7-intermediate-dense-weight': tensor([[ 6.9200e-05,  7.6763e-05,  9.0740e-05,  ...,  7.4800e-05,\n",
              "           6.9642e-05,  8.1375e-05],\n",
              "         [ 1.7830e-05,  1.9778e-05,  2.3381e-05,  ...,  1.9273e-05,\n",
              "           1.7945e-05,  2.0971e-05],\n",
              "         [ 5.2116e-05,  5.7811e-05,  6.8335e-05,  ...,  5.6333e-05,\n",
              "           5.2448e-05,  6.1283e-05],\n",
              "         ...,\n",
              "         [ 2.8657e-05,  3.1789e-05,  3.7576e-05,  ...,  3.0976e-05,\n",
              "           2.8839e-05,  3.3697e-05],\n",
              "         [-8.6847e-06, -9.6339e-06, -1.1390e-05,  ..., -9.3893e-06,\n",
              "          -8.7425e-06, -1.0220e-05],\n",
              "         [-2.2098e-05, -2.4507e-05, -2.8975e-05,  ..., -2.3881e-05,\n",
              "          -2.2236e-05, -2.5981e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-7-output-dense-weight': tensor([[-1.1293e-05, -1.3620e-05, -1.2674e-05,  ..., -1.0810e-05,\n",
              "          -1.0588e-05, -1.0137e-05],\n",
              "         [-8.9677e-07, -1.0788e-06, -1.0047e-06,  ..., -8.5539e-07,\n",
              "          -8.3882e-07, -8.0281e-07],\n",
              "         [-3.8974e-06, -4.7001e-06, -4.3739e-06,  ..., -3.7303e-06,\n",
              "          -3.6540e-06, -3.4981e-06],\n",
              "         ...,\n",
              "         [-2.5467e-06, -3.0696e-06, -2.8555e-06,  ..., -2.4355e-06,\n",
              "          -2.3859e-06, -2.2838e-06],\n",
              "         [ 1.5742e-05,  1.8984e-05,  1.7665e-05,  ...,  1.5066e-05,\n",
              "           1.4759e-05,  1.4129e-05],\n",
              "         [ 2.0872e-08,  2.4932e-08,  2.3947e-08,  ...,  1.9498e-08,\n",
              "           1.9856e-08,  1.8839e-08]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-8-attention-output-dense-weight': tensor([[ 5.4404e-06,  6.2997e-06,  6.3290e-06,  ...,  5.8554e-06,\n",
              "           5.7579e-06,  5.8209e-06],\n",
              "         [-1.3640e-05, -1.5794e-05, -1.5870e-05,  ..., -1.4689e-05,\n",
              "          -1.4442e-05, -1.4595e-05],\n",
              "         [-1.2725e-04, -1.4735e-04, -1.4803e-04,  ..., -1.3692e-04,\n",
              "          -1.3465e-04, -1.3614e-04],\n",
              "         ...,\n",
              "         [ 9.9016e-05,  1.1466e-04,  1.1518e-04,  ...,  1.0651e-04,\n",
              "           1.0475e-04,  1.0592e-04],\n",
              "         [ 3.5492e-05,  4.1101e-05,  4.1289e-05,  ...,  3.8192e-05,\n",
              "           3.7557e-05,  3.7970e-05],\n",
              "         [-3.7048e-05, -4.2908e-05, -4.3098e-05,  ..., -3.9830e-05,\n",
              "          -3.9177e-05, -3.9614e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-8-attention-self-key-weight': tensor([[-2.5869e-05, -2.5548e-05, -2.6897e-05,  ..., -2.7157e-05,\n",
              "          -2.6516e-05, -2.4575e-05],\n",
              "         [ 5.3941e-06,  5.3261e-06,  5.6090e-06,  ...,  5.6615e-06,\n",
              "           5.5279e-06,  5.1226e-06],\n",
              "         [ 9.8933e-05,  9.7678e-05,  1.0287e-04,  ...,  1.0383e-04,\n",
              "           1.0139e-04,  9.3963e-05],\n",
              "         ...,\n",
              "         [-2.5966e-05, -2.5645e-05, -2.7004e-05,  ..., -2.7261e-05,\n",
              "          -2.6618e-05, -2.4671e-05],\n",
              "         [-3.6615e-05, -3.6151e-05, -3.8071e-05,  ..., -3.8429e-05,\n",
              "          -3.7525e-05, -3.4778e-05],\n",
              "         [-2.9849e-05, -2.9471e-05, -3.1036e-05,  ..., -3.1327e-05,\n",
              "          -3.0590e-05, -2.8349e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-8-attention-self-query-weight': tensor([[ 1.0771e-04,  1.2982e-04,  1.2026e-04,  ...,  1.4247e-04,\n",
              "           1.1599e-04,  1.1333e-04],\n",
              "         [ 2.3818e-05,  2.8707e-05,  2.6592e-05,  ...,  3.1503e-05,\n",
              "           2.5649e-05,  2.5062e-05],\n",
              "         [ 7.3350e-05,  8.8408e-05,  8.1891e-05,  ...,  9.7019e-05,\n",
              "           7.8990e-05,  7.7182e-05],\n",
              "         ...,\n",
              "         [-9.3673e-05, -1.1290e-04, -1.0458e-04,  ..., -1.2390e-04,\n",
              "          -1.0088e-04, -9.8565e-05],\n",
              "         [-6.1802e-05, -7.4484e-05, -6.8996e-05,  ..., -8.1737e-05,\n",
              "          -6.6549e-05, -6.5023e-05],\n",
              "         [ 5.0547e-05,  6.0923e-05,  5.6434e-05,  ...,  6.6856e-05,\n",
              "           5.4433e-05,  5.3186e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-8-attention-self-value-weight': tensor([[-1.0976e-04, -1.2081e-04, -1.2982e-04,  ..., -1.0003e-04,\n",
              "          -1.1760e-04, -1.2667e-04],\n",
              "         [ 6.9651e-05,  7.6693e-05,  8.2401e-05,  ...,  6.3503e-05,\n",
              "           7.4596e-05,  8.0334e-05],\n",
              "         [-5.7050e-06, -6.2793e-06, -6.7487e-06,  ..., -5.1998e-06,\n",
              "          -6.1126e-06, -6.5835e-06],\n",
              "         ...,\n",
              "         [ 6.6310e-05,  7.2982e-05,  7.8428e-05,  ...,  6.0433e-05,\n",
              "           7.1047e-05,  7.6525e-05],\n",
              "         [-2.0858e-05, -2.2934e-05, -2.4664e-05,  ..., -1.8992e-05,\n",
              "          -2.2342e-05, -2.4061e-05],\n",
              "         [ 2.9176e-05,  3.2113e-05,  3.4509e-05,  ...,  2.6591e-05,\n",
              "           3.1261e-05,  3.3671e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-8-intermediate-dense-weight': tensor([[-9.1386e-06, -7.3474e-06, -7.9431e-06,  ..., -8.1328e-06,\n",
              "          -7.2544e-06, -7.9079e-06],\n",
              "         [-1.2968e-05, -1.0427e-05, -1.1268e-05,  ..., -1.1546e-05,\n",
              "          -1.0296e-05, -1.1228e-05],\n",
              "         [ 6.3646e-05,  5.1174e-05,  5.5322e-05,  ...,  5.6646e-05,\n",
              "           5.0536e-05,  5.5116e-05],\n",
              "         ...,\n",
              "         [ 8.7328e-06,  7.0331e-06,  7.5522e-06,  ...,  7.7641e-06,\n",
              "           6.9560e-06,  7.6266e-06],\n",
              "         [-8.4515e-05, -6.7955e-05, -7.3463e-05,  ..., -7.5222e-05,\n",
              "          -6.7109e-05, -7.3192e-05],\n",
              "         [-3.0503e-05, -2.4535e-05, -2.6519e-05,  ..., -2.7161e-05,\n",
              "          -2.4235e-05, -2.6448e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-8-output-dense-weight': tensor([[-1.9782e-05, -1.7173e-05, -1.5413e-05,  ..., -1.7761e-05,\n",
              "          -1.7418e-05, -1.8469e-05],\n",
              "         [ 1.4231e-05,  1.2354e-05,  1.1088e-05,  ...,  1.2773e-05,\n",
              "           1.2530e-05,  1.3286e-05],\n",
              "         [-1.5865e-05, -1.3772e-05, -1.2361e-05,  ..., -1.4243e-05,\n",
              "          -1.3968e-05, -1.4811e-05],\n",
              "         ...,\n",
              "         [ 2.3872e-05,  2.0722e-05,  1.8599e-05,  ...,  2.1424e-05,\n",
              "           2.1018e-05,  2.2286e-05],\n",
              "         [-8.9611e-06, -7.7794e-06, -6.9823e-06,  ..., -8.0461e-06,\n",
              "          -7.8902e-06, -8.3665e-06],\n",
              "         [-4.4971e-06, -3.9046e-06, -3.5047e-06,  ..., -4.0432e-06,\n",
              "          -3.9609e-06, -4.1992e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-9-attention-output-dense-weight': tensor([[ 4.2467e-05,  3.7655e-05,  3.2986e-05,  ...,  3.5366e-05,\n",
              "           3.4835e-05,  3.7007e-05],\n",
              "         [ 1.1079e-04,  9.8276e-05,  8.6077e-05,  ...,  9.2292e-05,\n",
              "           9.0911e-05,  9.6576e-05],\n",
              "         [-6.0308e-05, -5.3476e-05, -4.6849e-05,  ..., -5.0224e-05,\n",
              "          -4.9472e-05, -5.2555e-05],\n",
              "         ...,\n",
              "         [-4.7685e-05, -4.2293e-05, -3.7048e-05,  ..., -3.9720e-05,\n",
              "          -3.9124e-05, -4.1565e-05],\n",
              "         [-8.3514e-05, -7.4077e-05, -6.4904e-05,  ..., -6.9571e-05,\n",
              "          -6.8532e-05, -7.2808e-05],\n",
              "         [-6.2825e-05, -5.5709e-05, -4.8800e-05,  ..., -5.2322e-05,\n",
              "          -5.1535e-05, -5.4749e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-9-attention-self-key-weight': tensor([[ 6.3585e-05,  5.1400e-05,  5.4190e-05,  ...,  5.0895e-05,\n",
              "           6.2644e-05,  5.2652e-05],\n",
              "         [ 2.6257e-05,  2.1226e-05,  2.2378e-05,  ...,  2.1017e-05,\n",
              "           2.5869e-05,  2.1742e-05],\n",
              "         [-8.9505e-05, -7.2352e-05, -7.6281e-05,  ..., -7.1642e-05,\n",
              "          -8.8180e-05, -7.4116e-05],\n",
              "         ...,\n",
              "         [ 5.3968e-05,  4.3620e-05,  4.5991e-05,  ...,  4.3198e-05,\n",
              "           5.3164e-05,  4.4681e-05],\n",
              "         [-5.3879e-05, -4.3566e-05, -4.5889e-05,  ..., -4.3120e-05,\n",
              "          -5.3080e-05, -4.4636e-05],\n",
              "         [ 2.0632e-05,  1.6657e-05,  1.7588e-05,  ...,  1.6502e-05,\n",
              "           2.0322e-05,  1.7085e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-9-attention-self-query-weight': tensor([[-2.3636e-06, -2.0200e-06, -2.2863e-06,  ..., -2.4338e-06,\n",
              "          -2.1021e-06, -2.6647e-06],\n",
              "         [-3.2884e-05, -2.8100e-05, -3.1848e-05,  ..., -3.3893e-05,\n",
              "          -2.9250e-05, -3.7031e-05],\n",
              "         [-2.0499e-05, -1.7516e-05, -1.9851e-05,  ..., -2.1129e-05,\n",
              "          -1.8233e-05, -2.3080e-05],\n",
              "         ...,\n",
              "         [ 1.2774e-04,  1.0915e-04,  1.2369e-04,  ...,  1.3166e-04,\n",
              "           1.1362e-04,  1.4383e-04],\n",
              "         [-2.4791e-05, -2.1182e-05, -2.4002e-05,  ..., -2.5552e-05,\n",
              "          -2.2048e-05, -2.7904e-05],\n",
              "         [ 6.7305e-05,  5.7507e-05,  6.5190e-05,  ...,  6.9382e-05,\n",
              "           5.9863e-05,  7.5766e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-9-attention-self-value-weight': tensor([[ 3.6269e-05,  3.3840e-05,  3.5639e-05,  ...,  3.5468e-05,\n",
              "           3.2132e-05,  3.3545e-05],\n",
              "         [ 5.2361e-05,  4.8836e-05,  5.1371e-05,  ...,  5.1236e-05,\n",
              "           4.6344e-05,  4.8312e-05],\n",
              "         [ 5.5287e-06,  5.1524e-06,  5.4147e-06,  ...,  5.4114e-06,\n",
              "           4.8866e-06,  5.0866e-06],\n",
              "         ...,\n",
              "         [ 1.0528e-04,  9.8189e-05,  1.0330e-04,  ...,  1.0301e-04,\n",
              "           9.3181e-05,  9.7144e-05],\n",
              "         [ 6.5592e-05,  6.1175e-05,  6.4351e-05,  ...,  6.4187e-05,\n",
              "           5.8053e-05,  6.0511e-05],\n",
              "         [-3.8078e-05, -3.5513e-05, -3.7357e-05,  ..., -3.7268e-05,\n",
              "          -3.3697e-05, -3.5115e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-9-intermediate-dense-weight': tensor([[-1.0314e-04, -8.2626e-05, -1.0773e-04,  ..., -8.9675e-05,\n",
              "          -9.4589e-05, -7.9175e-05],\n",
              "         [ 9.7855e-08,  7.8693e-08,  1.0438e-07,  ...,  8.4929e-08,\n",
              "           8.9681e-08,  8.0168e-08],\n",
              "         [-3.0497e-05, -2.4430e-05, -3.1856e-05,  ..., -2.6517e-05,\n",
              "          -2.7969e-05, -2.3407e-05],\n",
              "         ...,\n",
              "         [-6.0117e-05, -4.8161e-05, -6.2794e-05,  ..., -5.2271e-05,\n",
              "          -5.5135e-05, -4.6147e-05],\n",
              "         [ 2.0786e-06,  1.6651e-06,  2.1742e-06,  ...,  1.8066e-06,\n",
              "           1.9065e-06,  1.6027e-06],\n",
              "         [-1.9252e-05, -1.5417e-05, -2.0109e-05,  ..., -1.6740e-05,\n",
              "          -1.7654e-05, -1.4767e-05]], grad_fn=<MulBackward0>),\n",
              " 'bert-encoder-layer-9-output-dense-weight': tensor([[2.8190e-05, 2.1000e-05, 2.0337e-05,  ..., 2.1794e-05, 2.2525e-05,\n",
              "          2.5930e-05],\n",
              "         [2.6528e-06, 1.9765e-06, 1.9139e-06,  ..., 2.0511e-06, 2.1195e-06,\n",
              "          2.4402e-06],\n",
              "         [2.2109e-05, 1.6472e-05, 1.5951e-05,  ..., 1.7092e-05, 1.7668e-05,\n",
              "          2.0337e-05],\n",
              "         ...,\n",
              "         [8.7821e-06, 6.5419e-06, 6.3358e-06,  ..., 6.7883e-06, 7.0193e-06,\n",
              "          8.0783e-06],\n",
              "         [1.4945e-05, 1.1134e-05, 1.0782e-05,  ..., 1.1555e-05, 1.1942e-05,\n",
              "          1.3747e-05],\n",
              "         [7.7007e-06, 5.7350e-06, 5.5546e-06,  ..., 5.9537e-06, 6.1523e-06,\n",
              "          7.0834e-06]], grad_fn=<MulBackward0>),\n",
              " 'bert-pooler-dense-weight': tensor([[4.6104e-05, 6.1691e-05, 5.0074e-05,  ..., 5.6118e-05, 5.3917e-05,\n",
              "          5.8515e-05],\n",
              "         [2.7422e-06, 3.5953e-06, 2.9552e-06,  ..., 3.3682e-06, 3.1674e-06,\n",
              "          3.3827e-06],\n",
              "         [1.2577e-05, 1.6880e-05, 1.3676e-05,  ..., 1.5287e-05, 1.4735e-05,\n",
              "          1.6030e-05],\n",
              "         ...,\n",
              "         [7.3426e-05, 9.8341e-05, 7.9777e-05,  ..., 8.9338e-05, 8.5916e-05,\n",
              "          9.3310e-05],\n",
              "         [3.3861e-05, 4.5380e-05, 3.6799e-05,  ..., 4.1187e-05, 3.9636e-05,\n",
              "          4.3069e-05],\n",
              "         [6.4071e-05, 8.5895e-05, 6.9639e-05,  ..., 7.7921e-05, 7.5014e-05,\n",
              "          8.1532e-05]], grad_fn=<MulBackward0>),\n",
              " 'classifier-weight': tensor([[ 6.5317e-05,  3.0625e-05, -7.9748e-06, -2.7906e-06,  4.5780e-05,\n",
              "           3.3976e-05, -2.0726e-05,  1.9890e-05,  4.3665e-06,  8.4410e-05,\n",
              "           8.7179e-06,  4.2610e-05, -2.1934e-05,  3.7457e-05, -2.6218e-05,\n",
              "           5.1306e-05,  2.9848e-05,  4.5635e-05,  7.2402e-06,  6.9640e-05,\n",
              "           3.8749e-06,  1.1309e-05, -1.0563e-05,  1.4266e-05,  1.1288e-05,\n",
              "           4.3724e-05,  5.0100e-05, -2.3245e-05, -2.9451e-05, -4.0026e-06,\n",
              "           6.0047e-05,  1.8285e-05,  7.6361e-05,  3.7015e-05, -6.6241e-06,\n",
              "           8.0213e-05,  2.6879e-05,  5.6851e-05,  2.2435e-05,  2.5931e-05,\n",
              "           6.7984e-05,  1.3873e-05, -2.3957e-05,  5.2056e-05,  3.7497e-05,\n",
              "           3.8502e-05,  7.9664e-05,  1.9769e-05,  6.5340e-05,  5.9263e-05,\n",
              "           5.3814e-05,  6.7148e-05,  2.2994e-05,  9.9152e-06,  4.9288e-06,\n",
              "          -1.6625e-06,  3.1602e-05,  1.8017e-05,  2.8394e-05,  4.9393e-05,\n",
              "           5.4748e-05,  2.1674e-05,  5.4920e-06,  7.4201e-05,  6.0050e-05,\n",
              "           6.0381e-05,  2.6527e-05,  3.4373e-05,  2.3559e-05,  2.7245e-05,\n",
              "          -1.1374e-05,  1.7389e-05,  9.2294e-06,  6.2104e-05,  5.6166e-05,\n",
              "           1.7427e-05,  3.7209e-05, -2.9751e-05,  4.0734e-05,  7.4585e-05,\n",
              "           6.5059e-05,  4.6165e-05,  1.6274e-05, -1.9481e-05,  6.2978e-05,\n",
              "           7.1426e-05,  1.1690e-05,  2.6000e-05,  7.4525e-05,  2.1749e-05,\n",
              "           2.2301e-05,  3.9138e-05,  5.9595e-05,  1.4053e-05,  3.6205e-05,\n",
              "           2.3859e-05,  3.6982e-05, -1.0867e-05,  2.4131e-05,  1.9224e-05,\n",
              "           2.6040e-05,  3.4839e-05,  2.2210e-05,  4.2848e-05,  2.3371e-05,\n",
              "           4.2142e-05,  4.7951e-05,  1.7986e-05,  4.2214e-05, -5.0153e-06,\n",
              "           1.7316e-05,  3.3086e-05,  1.6329e-05,  7.9750e-05, -1.9214e-06,\n",
              "           3.4576e-05,  7.6849e-05,  3.6377e-05,  7.0227e-05, -4.2271e-06,\n",
              "           2.0908e-05,  3.1434e-05, -3.1976e-05, -6.3950e-06,  1.5764e-05,\n",
              "           2.4361e-05, -1.3381e-05,  8.4913e-05,  3.2345e-05,  3.0245e-05,\n",
              "           2.7388e-06,  2.7171e-05,  8.2329e-05,  7.3489e-05, -1.5236e-06,\n",
              "          -1.9908e-05,  2.4455e-05, -2.6383e-05,  3.2425e-05, -1.7978e-05,\n",
              "           1.3003e-05,  1.9962e-05,  5.0455e-05,  3.6981e-05,  1.5784e-05,\n",
              "           6.1142e-06,  5.3283e-05,  1.9043e-05,  1.0880e-05,  4.1148e-05,\n",
              "           1.0941e-05,  3.9124e-05,  1.1182e-06,  7.5298e-05,  4.3562e-05,\n",
              "          -2.1378e-05, -1.7834e-07, -1.3148e-05, -1.1347e-05,  3.1147e-05,\n",
              "           4.0779e-05, -1.9079e-05,  1.5807e-05,  1.4463e-05,  2.2797e-05,\n",
              "           1.2159e-05,  3.5136e-05,  7.2612e-06,  6.4356e-05,  5.9812e-06,\n",
              "           1.2184e-05,  2.5689e-05, -1.4725e-05,  7.7927e-05,  3.1553e-05,\n",
              "           9.5342e-06, -2.4258e-05, -7.2832e-06,  2.1387e-05,  3.5278e-05,\n",
              "           3.6309e-05,  2.6696e-05,  7.8027e-05, -1.8813e-05,  3.2806e-05,\n",
              "           1.6738e-05, -7.9047e-06,  3.8830e-05,  6.8986e-05,  4.8803e-05,\n",
              "          -1.6793e-05,  1.5661e-05,  6.8128e-05,  2.1571e-05,  4.5965e-05,\n",
              "           4.2828e-05, -3.2630e-07, -7.0076e-08,  3.5792e-05,  4.3296e-05,\n",
              "           2.2848e-05, -2.4659e-05, -3.6439e-05, -7.5046e-06,  6.1463e-05,\n",
              "          -3.2177e-06,  6.7832e-05,  4.0236e-05,  2.5558e-05,  1.2438e-05,\n",
              "           2.4671e-05, -2.4201e-05,  1.3534e-05,  3.2157e-05,  7.2748e-05,\n",
              "           7.6112e-05,  2.6154e-05,  7.8511e-05,  2.2863e-05,  5.7546e-05,\n",
              "           1.8896e-05, -1.1523e-05,  3.5997e-05,  4.6703e-06,  8.4045e-05,\n",
              "           7.2696e-05,  1.3784e-05,  2.6920e-05,  1.3965e-05,  3.5079e-05,\n",
              "           2.1951e-05,  5.2698e-05,  4.6768e-05, -1.3629e-05, -2.7699e-05,\n",
              "          -1.2528e-05,  5.3759e-05, -1.7286e-05,  3.6857e-05, -1.8223e-05,\n",
              "           5.4071e-05, -1.7240e-05,  4.6635e-05,  9.8749e-06,  6.8007e-05,\n",
              "          -4.1601e-06,  7.4802e-05,  2.8402e-06,  2.7235e-05,  7.0910e-05,\n",
              "           5.6078e-05,  1.2933e-05,  1.4861e-05, -1.5946e-05,  4.5022e-05,\n",
              "          -2.1812e-05,  2.4699e-05,  7.4064e-05, -6.8823e-06,  2.0919e-05,\n",
              "           7.6723e-05,  4.6021e-05,  1.3948e-05,  6.3663e-05,  3.5108e-05,\n",
              "           3.9333e-05,  7.8210e-05, -1.7120e-05,  2.1986e-05, -1.9844e-05,\n",
              "           1.4108e-05,  7.4872e-05,  3.4065e-05,  6.6327e-05,  3.9722e-05,\n",
              "           2.6992e-05, -5.8978e-06,  3.6791e-05,  7.0675e-05,  9.4007e-06,\n",
              "           3.1406e-06,  8.4547e-06, -9.5690e-06, -2.7003e-05, -2.6479e-05,\n",
              "          -1.8149e-05, -1.6098e-05, -2.1619e-05,  2.3718e-05,  1.0072e-05,\n",
              "          -2.3698e-05,  1.9690e-05,  8.1808e-05,  7.5808e-05,  4.4868e-05,\n",
              "           7.2716e-06,  8.6576e-05,  2.8594e-05,  2.3411e-05,  7.3766e-05,\n",
              "           5.4354e-05, -2.5903e-05, -2.2576e-05,  8.2313e-05, -1.4112e-05,\n",
              "          -3.7902e-05,  4.2880e-05,  4.0852e-05,  3.1959e-05, -2.1593e-05,\n",
              "           1.3970e-05,  9.0082e-06,  3.4166e-05,  1.7105e-05,  7.5096e-06,\n",
              "           6.9419e-05, -3.4124e-06, -2.1729e-06,  4.5009e-05,  1.9274e-05,\n",
              "           5.6807e-05,  6.6943e-05,  4.8564e-05,  2.6694e-05,  4.1345e-05,\n",
              "           8.0210e-05,  2.8108e-05,  5.0535e-05,  7.6852e-06,  2.3827e-05,\n",
              "           2.1424e-05,  5.9536e-05,  1.9711e-05,  5.6775e-05,  6.7580e-06,\n",
              "           2.6360e-06,  7.4286e-05,  5.2667e-05,  2.0248e-05,  5.6599e-05,\n",
              "          -5.4371e-07,  6.9091e-05, -2.8781e-05,  3.5477e-05,  4.3438e-05,\n",
              "          -3.4450e-05,  5.5281e-05,  6.7545e-05,  1.5068e-05,  2.1592e-05,\n",
              "           2.5939e-05, -2.3681e-05,  1.6617e-05,  7.7562e-05,  3.7859e-05,\n",
              "           2.7997e-05,  3.7630e-05,  3.5161e-05, -2.6103e-05,  3.3892e-05,\n",
              "          -7.8781e-06,  1.4000e-07, -2.5548e-05,  7.9341e-05,  5.1556e-05,\n",
              "           7.3687e-05,  7.5443e-05, -1.9888e-05, -2.3846e-05,  2.3214e-05,\n",
              "           5.3007e-05,  2.1569e-05,  1.1655e-05,  2.0927e-05,  8.1954e-05,\n",
              "          -2.6209e-07,  3.7358e-06,  3.0593e-05, -2.1383e-05,  6.7787e-05,\n",
              "           3.7058e-05,  9.5680e-06,  1.1978e-05, -2.9062e-06,  5.6169e-05,\n",
              "           7.3527e-06,  3.1470e-05,  2.7392e-05,  3.8219e-05,  9.8512e-06,\n",
              "           7.2193e-05,  3.7032e-05, -3.0420e-05,  1.1142e-05,  5.8627e-05,\n",
              "           3.1346e-05,  3.3010e-05,  4.7558e-05,  1.8604e-05,  9.5056e-06,\n",
              "           3.3431e-05,  5.9844e-05,  4.8814e-05,  6.3236e-05,  8.4493e-05,\n",
              "          -4.0311e-06,  2.1091e-05,  3.7136e-05, -1.9779e-05,  1.9236e-05,\n",
              "           2.5894e-05,  3.7907e-05,  4.4190e-05,  2.7154e-05, -7.6644e-07,\n",
              "           5.7399e-05, -3.1624e-05,  3.3482e-05,  1.5943e-05, -1.7348e-05,\n",
              "          -3.5556e-06,  3.6849e-05,  6.0991e-05,  2.5219e-05,  7.5290e-05,\n",
              "           2.0877e-05,  7.7125e-05, -3.3433e-05,  6.2304e-05,  1.5818e-05,\n",
              "           2.4048e-05,  4.0121e-05, -2.3859e-05, -3.7720e-06, -1.7762e-06,\n",
              "           5.8529e-05, -2.1542e-05,  2.3223e-05,  5.9170e-05,  3.6069e-05,\n",
              "           2.2194e-05, -1.4704e-05,  3.5225e-05,  1.5006e-05,  6.8639e-05,\n",
              "           5.6118e-05,  4.6835e-05,  8.0735e-05,  7.5137e-05, -1.3529e-05,\n",
              "          -9.9814e-06,  2.7143e-05,  9.2566e-06,  5.4285e-05,  5.6641e-05,\n",
              "           2.5145e-05,  2.8523e-05,  7.2034e-05, -3.8192e-06,  3.3983e-05,\n",
              "           1.2301e-05,  3.7040e-05,  9.1487e-06,  6.3400e-05, -2.1419e-05,\n",
              "          -9.4847e-06,  9.5448e-06,  2.5864e-05,  6.0864e-05, -7.4545e-06,\n",
              "           6.8880e-05, -1.0517e-05,  2.6456e-05, -2.6292e-05,  4.0698e-05,\n",
              "           4.6514e-05, -8.7715e-06, -6.5155e-06,  2.6436e-05,  2.0698e-05,\n",
              "           6.0432e-05,  2.5949e-05, -8.6135e-06, -8.6297e-06,  6.9373e-05,\n",
              "           3.5504e-05,  4.5835e-06,  6.2355e-05,  5.5652e-05, -1.0267e-05,\n",
              "           3.9442e-05,  3.1120e-05,  2.4048e-05,  1.9121e-05, -3.5719e-05,\n",
              "           2.6427e-05,  2.9099e-05,  4.4804e-05,  2.1523e-05,  3.7254e-05,\n",
              "           5.7720e-05, -2.7236e-05,  8.9396e-06,  4.5989e-05,  8.0261e-05,\n",
              "          -1.0511e-05,  7.5422e-05, -1.5689e-05, -6.9183e-06,  7.9602e-05,\n",
              "           4.6368e-06,  2.5604e-05,  2.9020e-05, -1.4465e-05,  2.9866e-05,\n",
              "           3.9027e-05,  2.4221e-05,  2.1623e-05, -2.9167e-05,  3.9248e-05,\n",
              "           8.0744e-05,  4.6387e-05,  1.7463e-05,  7.9673e-05,  2.4839e-05,\n",
              "           4.7959e-05,  2.8081e-05,  2.6896e-05, -1.5989e-06, -2.4202e-05,\n",
              "           2.9121e-05,  7.6556e-05,  2.7344e-05,  3.2115e-05, -3.2976e-05,\n",
              "           1.8088e-05,  3.8642e-05,  7.4071e-05,  5.9874e-05,  3.7907e-05,\n",
              "          -8.2264e-06,  7.7946e-05, -3.1213e-05,  8.1492e-05,  1.2488e-05,\n",
              "          -2.8644e-05,  1.1322e-05,  6.3199e-05,  1.9517e-05,  4.4186e-05,\n",
              "           1.7518e-05,  3.7720e-06, -1.2386e-06,  7.8947e-05,  3.3148e-05,\n",
              "           3.1125e-05,  1.8755e-05,  3.3976e-05, -4.3060e-06, -1.8973e-06,\n",
              "           1.9969e-05,  3.6129e-05,  4.6329e-05,  2.6221e-05,  8.3819e-06,\n",
              "          -1.2547e-06,  3.6673e-05,  3.0370e-05,  2.4876e-05,  3.1189e-05,\n",
              "           6.3932e-05,  3.2972e-05,  3.0806e-05,  5.2327e-05, -5.4938e-06,\n",
              "           8.7094e-05,  5.9996e-05,  5.9585e-05,  3.2012e-05, -1.4949e-05,\n",
              "           3.4110e-05,  4.1034e-05,  5.6764e-05, -1.2990e-05, -2.3743e-05,\n",
              "          -2.2675e-06,  3.1591e-05,  5.0979e-06,  5.6620e-05,  2.5287e-05,\n",
              "           3.0072e-05,  2.2253e-05,  7.9166e-06, -1.1858e-05,  2.7573e-05,\n",
              "          -3.0151e-05,  2.5246e-05,  4.1476e-05,  7.2024e-05,  1.9423e-05,\n",
              "           3.1125e-05, -3.3181e-05,  7.3652e-05,  7.0609e-05,  1.4831e-05,\n",
              "           4.6735e-05,  7.1885e-05,  2.0640e-05,  3.1505e-05,  5.0376e-05,\n",
              "           6.2360e-07, -2.9753e-05, -2.0160e-05,  3.5465e-05,  1.2478e-05,\n",
              "           4.0851e-05,  4.0822e-05,  2.3601e-05,  6.7799e-05, -3.0411e-05,\n",
              "           2.3265e-05, -1.9379e-05, -2.2042e-05,  1.3117e-05, -2.1825e-05,\n",
              "           1.6712e-05, -3.0875e-06,  2.6406e-05, -3.0366e-05,  1.6134e-05,\n",
              "           6.9622e-05, -7.6272e-07,  8.6086e-05,  2.8800e-05,  7.6184e-05,\n",
              "           1.8206e-05,  2.7980e-05, -1.1321e-05,  3.3142e-05, -1.3343e-05,\n",
              "          -9.4014e-06,  2.2695e-05,  9.1513e-06, -1.1964e-05,  1.1116e-05,\n",
              "           6.9903e-05,  7.2918e-05,  7.9949e-05,  2.0410e-05,  4.4214e-05,\n",
              "           2.8136e-05,  8.9788e-06,  1.7428e-05,  1.4242e-05,  1.2078e-05,\n",
              "           7.9769e-05, -2.6439e-05,  9.1322e-06,  5.9889e-05, -2.8513e-05,\n",
              "           2.8454e-05,  2.4366e-05,  1.9355e-05,  8.1240e-05,  6.9671e-05,\n",
              "           4.2177e-05,  3.4785e-05, -1.4907e-05,  4.9441e-06, -2.1219e-05,\n",
              "           1.5389e-05,  4.6135e-05,  2.8406e-05, -4.6415e-06,  1.7757e-05,\n",
              "           8.2011e-05,  7.3605e-06, -2.9241e-06,  7.4074e-05, -2.4446e-05,\n",
              "           5.0286e-05,  3.5882e-05, -4.1460e-06, -4.1037e-06, -2.2239e-05,\n",
              "          -9.7783e-06,  1.8518e-06,  2.1328e-05,  6.7628e-06, -2.3833e-05,\n",
              "          -2.6013e-05, -2.6847e-05,  5.3911e-07, -1.1829e-05, -8.1588e-06,\n",
              "           1.1583e-05,  2.2478e-05,  7.4416e-05,  2.5951e-05,  3.4229e-05,\n",
              "           1.8173e-05,  2.2470e-05,  2.8253e-05,  8.4804e-05,  1.1008e-05,\n",
              "           2.5784e-05,  1.2507e-05,  4.2150e-05,  1.4868e-05,  3.9936e-05,\n",
              "           3.2019e-05,  5.5270e-05,  3.5714e-05,  7.9415e-06,  1.0448e-05,\n",
              "          -2.2371e-05,  3.2384e-05,  2.7692e-05,  5.1780e-05,  2.4947e-05,\n",
              "          -3.6568e-06,  7.2988e-05, -1.4801e-05,  2.4333e-05,  1.4214e-06,\n",
              "           4.4033e-05,  3.2644e-05,  1.0341e-05,  5.7379e-05,  4.0493e-05,\n",
              "           3.7032e-05,  5.7268e-05, -1.5988e-05,  2.2958e-05,  4.0924e-05,\n",
              "           4.0403e-05,  1.2664e-06,  1.5292e-05,  1.2730e-05,  4.4246e-06,\n",
              "           2.3846e-06,  2.1304e-05,  3.2519e-05,  1.5322e-05,  3.2767e-05,\n",
              "           8.6503e-05,  1.2825e-05, -1.7101e-06,  4.8725e-05,  7.1173e-06,\n",
              "           6.1579e-05,  1.1361e-05,  7.7714e-05,  2.8357e-05,  4.1088e-05,\n",
              "           4.7705e-05,  5.2937e-05,  3.1755e-05,  1.5467e-05, -1.0727e-05,\n",
              "           4.6821e-05, -1.9128e-05,  7.3770e-06, -7.5064e-06,  8.0798e-06,\n",
              "           1.8589e-06,  5.4370e-05, -1.7887e-05]], grad_fn=<MulBackward0>)}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW_AhNDaTZXN",
        "outputId": "37ab85ac-ee5e-4b75-84c2-02ff0829fdc7"
      },
      "source": [
        "x = torch.randn(2, 1, 4)\n",
        "print(x[0])\n",
        "print(x[1])\n",
        "print(x.flatten().size())"
      ],
      "id": "oW_AhNDaTZXN",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9363, -0.4891, -0.2635, -0.6071]])\n",
            "tensor([[ 1.2396, -1.1095, -1.3711,  0.7535]])\n",
            "torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uq13Y1HiTZOl"
      },
      "source": [
        "# output = bert_fc_model(id.unsqueeze(0), type_id.unsqueeze(0), mask.unsqueeze(0)).unsqueeze(0)\n",
        "# y = torch.FloatTensor(1)\n",
        "# bert_loss = torch.nn.BCELoss()\n",
        "# loss = bert_loss(output, y)\n",
        "# loss.backward()\n",
        "# bert_fc_model.zero_grad()\n",
        "# for name, param in bert_fc_model.named_parameters():\n",
        "  # print(name)\n",
        "  # print(param.grad)"
      ],
      "id": "uq13Y1HiTZOl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5KgKyKMTZHE"
      },
      "source": [
        ""
      ],
      "id": "I5KgKyKMTZHE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-unx_pATY-g"
      },
      "source": [
        ""
      ],
      "id": "z-unx_pATY-g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oljr9s2TY1N"
      },
      "source": [
        ""
      ],
      "id": "8oljr9s2TY1N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J2-blDZTYuH"
      },
      "source": [
        ""
      ],
      "id": "_J2-blDZTYuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qew6PnSiTYeo"
      },
      "source": [
        ""
      ],
      "id": "Qew6PnSiTYeo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiqNHhHSTAPr",
        "outputId": "ba94efe9-30ba-4187-a2d7-daaf25ee1e98"
      },
      "source": [
        "# # authors_model['state_dict']\n",
        "# authors_model = torch.load('data/FC_model.ckpt')\n",
        "\n",
        "# def modify_authors_state_dict(state_dict):\n",
        "#     \"\"\"The state dicts prefixes don't match (ours is bert.xyz, \n",
        "#     their's is model.model.xyz. This function alters the \n",
        "#     naming in their state_dict to match\"\"\"\n",
        "#     from collections import OrderedDict\n",
        "#     new_state_dict = OrderedDict()\n",
        "#     for x in state_dict.items():\n",
        "#         # print(x)\n",
        "#         name = x[0]\n",
        "#         vals = x[1]\n",
        "#         if name[:11] == \"model.model\":\n",
        "#             new_name = \"bert\" + name[11:]\n",
        "#         else:\n",
        "#             new_name = name[6:]\n",
        "#         new_state_dict[new_name] = vals\n",
        "#     return new_state_dict\n",
        "\n",
        "# modified_state_dict = modify_authors_state_dict(authors_model['state_dict'])\n",
        "# # bert_fc_model.load_state_dict(authors_model['state_dict'])\n",
        "# bert_fc_model.load_state_dict(modified_state_dict)"
      ],
      "id": "WiqNHhHSTAPr",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w45y3lJYTAPs"
      },
      "source": [
        "# # Build the datasets\n",
        "# batch_size = 32\n",
        "# train_dataset = src.preprocess.FeverDataset(os.path.join(configs.DATA_DIR, 'fever', 'train.jsonl'))\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# dev_dataset = src.preprocess.FeverDataset(os.path.join(configs.DATA_DIR, 'fever', 'dev.jsonl'))\n",
        "# dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=True)"
      ],
      "id": "w45y3lJYTAPs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6XdsD0xTAPt",
        "outputId": "8764eb69-c2a3-45ec-8369-e4a60caf9e55"
      },
      "source": [
        "# bert_fc_model = bert_fc_model.to(device)\n",
        "\n",
        "# print(len(train_loader))\n",
        "# counter = 0\n",
        "# running_acc = 0\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for x,y in dev_loader:\n",
        "#         counter += 1\n",
        "#         print(counter)\n",
        "#         ids, type_ids, mask = x[\"input_ids\"], x[\"token_type_ids\"], x[\"attention_mask\"]\n",
        "#         ids, type_ids, mask = ids.squeeze(1), type_ids.squeeze(1), mask.squeeze(1)\n",
        "#         ids, type_ids, mask, y = ids.to(device), type_ids.to(device), mask.to(device), y.to(device)\n",
        "#         prob_pos = bert_fc_model(ids, type_ids, mask)\n",
        "#         preds = torch.round(prob_pos)\n",
        "#         accuracy = torch.sum(y == preds).item()\n",
        "#         running_acc += accuracy\n",
        "        \n",
        "#         print(\"Latest accuracy: \", accuracy/batch_size)\n",
        "#         print(\"Running accuracy: \", running_acc/(batch_size * counter))"
      ],
      "id": "Y6XdsD0xTAPt",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3281\n",
            "1\n",
            "Latest accuracy:  0.84375\n",
            "Running accuracy:  0.84375\n",
            "2\n",
            "Latest accuracy:  0.90625\n",
            "Running accuracy:  0.875\n",
            "3\n",
            "Latest accuracy:  0.75\n",
            "Running accuracy:  0.8333333333333334\n",
            "4\n",
            "Latest accuracy:  0.625\n",
            "Running accuracy:  0.78125\n",
            "5\n",
            "Latest accuracy:  0.75\n",
            "Running accuracy:  0.775\n",
            "6\n",
            "Latest accuracy:  0.8125\n",
            "Running accuracy:  0.78125\n",
            "7\n",
            "Latest accuracy:  0.71875\n",
            "Running accuracy:  0.7723214285714286\n",
            "8\n",
            "Latest accuracy:  0.8125\n",
            "Running accuracy:  0.77734375\n",
            "9\n",
            "Latest accuracy:  0.78125\n",
            "Running accuracy:  0.7777777777777778\n",
            "10\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2465/523181469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprob_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_fc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mrunning_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE39ffc8TAPu"
      },
      "source": [
        ""
      ],
      "id": "aE39ffc8TAPu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pAN1EwRTAPv"
      },
      "source": [
        ""
      ],
      "id": "8pAN1EwRTAPv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2VaNW-ATAPw"
      },
      "source": [
        ""
      ],
      "id": "t2VaNW-ATAPw",
      "execution_count": null,
      "outputs": []
    }
  ]
}